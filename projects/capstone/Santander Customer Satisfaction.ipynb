{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Santander Customer Satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "# sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "# data = train.drop('TARGET', axis=1).append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 371)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75818, 370)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.columns) - set(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of customers: 76020\n",
      "Number of satisfied customers: 73012\n",
      "Number of unsatisfied customers: 3008\n",
      "Satisfaction rate: 96.04%\n",
      "Number of features: 369\n"
     ]
    }
   ],
   "source": [
    "n_customers = len(train)\n",
    "n_unsatisfied = train.TARGET.sum()\n",
    "n_satisfied = (train.TARGET==0).sum()\n",
    "satisfaction_rate = float(n_satisfied)/n_customers\n",
    "\n",
    "features = test.columns.drop('ID').tolist()\n",
    "n_features = len(features)\n",
    "\n",
    "print(\"Total number of customers: {}\".format(n_customers))\n",
    "print(\"Number of satisfied customers: {}\".format(n_satisfied))\n",
    "print(\"Number of unsatisfied customers: {}\".format(n_unsatisfied))\n",
    "print(\"Satisfaction rate: {:.2f}%\".format(satisfaction_rate*100))\n",
    "print(\"Number of features: {}\".format(n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ID Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.ID)) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_id = train.ID\n",
    "test_id = test.ID\n",
    "train.drop('ID', inplace=True, axis=1)\n",
    "test.drop('ID', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Target Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exploring Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preparation\n",
    "- Missing values. `train.columns[train.isnull().any()].tolist()`\n",
    "- Outliers and strange values\n",
    "- Remove duplicated features\n",
    "- Remove useless features\n",
    "- Remove useless and duplicated features checking it not only on training set but also on testing set (but maybe they could be useful on validation set).\n",
    "- This dataset has some repeated instances but with both class label, so that instances are noise. What I made was extract that noisy instances from the complete dataset. After that, I splitted the resulting complete dataset in 5 partitions and I ran xgboost on each one. Then, I made a majority vote of the five resulting predictions for each noisy instance that I extrated at the begining. Finally, I inserted that instances with the real class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for c in train.select_dtypes('float'):\n",
    "#     sns.distplot(train[c]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_weird(dfs, weird = [-999999, 9999999999.00]):\n",
    "    for df in dfs:\n",
    "        modes = df.mode()\n",
    "        for col in df.columns:\n",
    "            if any([i in df[col].values for i in weird]):\n",
    "#                 df['weird_'+col] = df[col].isin(weird)\n",
    "                df[col].mask(df[col].isin(weird), modes[col][0], inplace=True)\n",
    "\n",
    "replace_weird([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_useless(dfs):\n",
    "    remove = []\n",
    "    for df in dfs:\n",
    "        for col in df.columns:\n",
    "            if df[col].std() == 0:\n",
    "                remove.append(col)\n",
    "    for df in dfs:\n",
    "        df.drop(set(remove), axis=1, inplace=True)\n",
    "\n",
    "remove_useless([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated columns on test set are also duplicated on training set.          \n",
      "All duplicated being removed from both sets\n"
     ]
    }
   ],
   "source": [
    "def duplicated(df):\n",
    "    duplicated = []\n",
    "    c = df.columns\n",
    "    for i in range(len(c)-1):\n",
    "        v = df[c[i]].values\n",
    "        for j in range(i+1,len(c)):\n",
    "            if np.array_equal(v,df[c[j]].values):\n",
    "                duplicated.append(c[j])\n",
    "    return set(duplicated)\n",
    "\n",
    "\n",
    "duplicated_train = duplicated(train)\n",
    "duplicated_test = duplicated(test)\n",
    "\n",
    "if duplicated_test.issubset(duplicated_train):\n",
    "    print('Duplicated columns on test set are also duplicated on training set.\\\n",
    "          \\nAll duplicated being removed from both sets')\n",
    "    train.drop(duplicated_train, axis=1, inplace=True)\n",
    "    test.drop(duplicated_train, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for c in train.select_dtypes('float'):\n",
    "#     sns.distplot(train[c]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Scaling\n",
    "- https://discuss.analyticsvidhya.com/t/methods-to-deal-with-zero-values-while-performing-log-transformation-of-variable/2431/3\n",
    "- Robust Scaler\n",
    "- Log Scaler\n",
    "- Scale Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "floats = train.select_dtypes('float').columns\n",
    "\n",
    "min_max_scaler.fit(train[floats])\n",
    "\n",
    "train[floats] = pd.DataFrame(min_max_scaler.transform(train[floats]), columns=floats)\n",
    "test[floats] = pd.DataFrame(min_max_scaler.transform(test[floats]), columns=floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for c in train.select_dtypes('float'):\n",
    "    sns.distplot(train[c]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit scaler to train+test\n",
    "# transform separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test with log scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True) # Convert a metric to a scorer.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.drop('TARGET', axis=1))\n",
    "y = np.array(train['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All zeros benchmark has ROC_AUC score of 0.5'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'All zeros benchmark has ROC_AUC score of {}'.format(roc_auc_score(train.TARGET, np.zeros_like(train.TARGET)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Baseline\n",
    "- Base Models\n",
    "- Grid Search\n",
    "- Model descriptions, strengths and weakness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5288501033449492"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "cross_val_score(nb_classifier, X, y, cv=3, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.635315698718681"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(class_weight={0:0.2, 1:0.8})\n",
    "cross_val_score(lr_classifier, X, y, cv=3, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5707285401791248"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "cross_val_score(dt_classifier, X, y, cv=3, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too slow\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# sv_classifier = SVC()\n",
    "# # cross_val_score(sv_classifier, X, y, cv=3, scoring='roc_auc').mean()\n",
    "\n",
    "print('Too slow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6478127425711916"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier()\n",
    "cross_val_score(mlp_classifier, X, y, cv=3, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6750333274976273"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=3)\n",
    "cross_val_score(rf_classifier, X, y, cv=5, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8365908042920411"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "cross_val_score(xgb_classifier, X, y, cv=3, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD3CAYAAAANMK+RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg4Ahvg/E6JnBr47qtgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cf8a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train.select_dtypes('int').nunique(), kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def OHE(train = train, test = test, limit=10):\n",
    "    oh_df = pd.concat([train, test])\n",
    "    for c in oh_df.select_dtypes(int):\n",
    "        if oh_df[c].nunique()>2 and oh_df[c].nunique()<limit:\n",
    "            for val in set(oh_df[c]):\n",
    "                train[c+'_oh_' + str(val)] = (train[c].values == val).astype(np.int)\n",
    "                test[c+'_oh_' + str(val)] = (test[c].values == val).astype(np.int)            \n",
    "#             train.drop(c, axis=1)\n",
    "#             test.drop(c, axis=1)\n",
    "\n",
    "OHE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Values sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def values_sum(df):\n",
    "    df['values_sum'] = df[features].sum(axis=1).astype(float)\n",
    "\n",
    "count_zeros(train)\n",
    "count_zeros(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Count zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_zeros(df):\n",
    "    df['zeros'] = (df[features]==0).sum(axis=1).astype(float)\n",
    "    \n",
    "count_zeros(train)\n",
    "count_zeros(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Value == Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_if_is_mode(df):\n",
    "    modes = df.mode()\n",
    "    for c in features:\n",
    "        df['is_mode_of_'+c] = (df[c]==modes[c][0]).astype(int)\n",
    "\n",
    "check_if_is_mode(train)\n",
    "check_if_is_mode(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD3CAYAAAAHQMOGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx+PXNv/ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ccd7390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train.dtypes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEHCAYAAAC+1b08AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIIOAIYg6ABgiP8HBcvs8kWGOOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cceb358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in train.select_dtypes('float').columns:\n",
    "    sns.distplot(train[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "floats = train.select_dtypes('float').columns\n",
    "min_max_scaler.fit(train[floats])\n",
    "\n",
    "train[floats] = pd.DataFrame(min_max_scaler.transform(train[floats]), columns=floats)\n",
    "test[floats] = pd.DataFrame(min_max_scaler.transform(test[floats]), columns=floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum non-zero value in data is 0.0000000069\n"
     ]
    }
   ],
   "source": [
    "minimum_float = pd.concat([train.select_dtypes(float), test.select_dtypes(float)]).replace(0,10).min().min()\n",
    "print('Minimum non-zero value in data is {:.10f}'.format(float(minimum_float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEFCAYAAAAsU2YoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d06cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in train.select_dtypes('float').columns:\n",
    "    sns.distplot(train[f].apply(lambda x: minimum_float/2 if x==0 else x).apply(np.log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEFCAYAAAD+A2xwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d154e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in train.select_dtypes('float').columns:\n",
    "    m = train[f].mode()[0]\n",
    "    sns.distplot(train[train[f]!=m][f].apply(lambda x: minimum_float/2 if x==0 else x).apply(np.log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "floats = train.select_dtypes('float').columns\n",
    "\n",
    "train[floats] = train[floats].applymap(lambda x: minimum_float/2 if x==0 else x).apply(np.log)\n",
    "test[floats] = test[floats].applymap(lambda x: minimum_float/2 if x==0 else x).apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(data):\n",
    "    n = len(data.drop('TARGET', axis=1).columns)\n",
    "    \n",
    "    X = np.array(data.drop('TARGET', axis=1))\n",
    "    y = np.array(data['TARGET'])\n",
    "    \n",
    "    lr_classifier.fit(X, y)\n",
    "    mlp_classifier.fit(X, y)\n",
    "    xgb_classifier.fit(X, y)\n",
    "\n",
    "    i_mlp = np.absolute(np.dot(mlp_classifier.coefs_[0],mlp_classifier.coefs_[1]).reshape(n))\n",
    "    i_lr = np.absolute(lr_classifier.coef_.reshape(n))\n",
    "    i_xgb = xgb_classifier.feature_importances_\n",
    "    \n",
    "    importances = pd.DataFrame({'mlp': i_mlp, 'lr': i_lr, 'xgb': i_xgb}, index=data.drop('TARGET', axis=1).columns)\n",
    "    importances = importances / importances.max()\n",
    "    importances['i'] = importances.sum(axis=1)\n",
    "    importances.sort_values('i', ascending=False, inplace=True)\n",
    "    \n",
    "    return importances\n",
    "\n",
    "importances = get_feature_importances(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEFCAYAAADzHRw3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111e982b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(importances.i, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(importances.loc[importances.i>importances.i.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEFCAYAAAAsU2YoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABhEvAHAIOINAAYRbwAwiHgDgEH/A5XMNw8R9f4xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c2177b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(importances.i.head(392), kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8367874716912175\n",
      "0.8211678389254847\n",
      "0.8195377738108588\n"
     ]
    }
   ],
   "source": [
    "X_top_features = np.array(train[importances.head(300).index])\n",
    "\n",
    "print(cross_val_score(xgb_classifier, X_top_features, y, cv=3, scoring='roc_auc').mean())\n",
    "print(cross_val_score(mlp_classifier, X_top_features, y, cv=3, scoring='roc_auc').mean())\n",
    "print(cross_val_score(lr_classifier, X_top_features, y, cv=3, scoring='roc_auc').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8367487924410447\n",
      "0.8033860335116362\n",
      "0.8167709947732843\n"
     ]
    }
   ],
   "source": [
    "X_top_features = np.array(train[importances.head(392).index])\n",
    "\n",
    "print(cross_val_score(xgb_classifier, X_top_features, y, cv=3, scoring='roc_auc').mean())\n",
    "print(cross_val_score(mlp_classifier, X_top_features, y, cv=3, scoring='roc_auc').mean())\n",
    "print(cross_val_score(lr_classifier, X_top_features, y, cv=3, scoring='roc_auc').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = train[list(importances.head(392).index)+['TARGET']]\n",
    "test = test[importances.head(392).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Parameter Optmization\n",
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.drop('TARGET', axis=1))\n",
    "y = np.array(train['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17086280\n",
      "Validation score: 0.960339\n",
      "Iteration 2, loss = 0.14850326\n",
      "Validation score: 0.960537\n",
      "Iteration 3, loss = 0.14553858\n",
      "Validation score: 0.959550\n",
      "Iteration 4, loss = 0.14428219\n",
      "Validation score: 0.960734\n",
      "Iteration 5, loss = 0.14177295\n",
      "Validation score: 0.960537\n",
      "Iteration 6, loss = 0.14291236\n",
      "Validation score: 0.960537\n",
      "Iteration 7, loss = 0.14072925\n",
      "Validation score: 0.960339\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21823595\n",
      "Validation score: 0.960142\n",
      "Iteration 2, loss = 0.15581898\n",
      "Validation score: 0.960142\n",
      "Iteration 3, loss = 0.14971687\n",
      "Validation score: 0.960734\n",
      "Iteration 4, loss = 0.14613597\n",
      "Validation score: 0.960537\n",
      "Iteration 5, loss = 0.14479719\n",
      "Validation score: 0.960339\n",
      "Iteration 6, loss = 0.14356776\n",
      "Validation score: 0.959945\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17985906\n",
      "Validation score: 0.961334\n",
      "Iteration 2, loss = 0.15368022\n",
      "Validation score: 0.961136\n",
      "Iteration 3, loss = 0.14910486\n",
      "Validation score: 0.961531\n",
      "Iteration 4, loss = 0.14753323\n",
      "Validation score: 0.961334\n",
      "Iteration 5, loss = 0.14682350\n",
      "Validation score: 0.961136\n",
      "Iteration 6, loss = 0.14448344\n",
      "Validation score: 0.961334\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17914127\n",
      "Validation score: 0.960142\n",
      "Iteration 2, loss = 0.15533326\n",
      "Validation score: 0.959945\n",
      "Iteration 3, loss = 0.14719657\n",
      "Validation score: 0.960142\n",
      "Iteration 4, loss = 0.14451105\n",
      "Validation score: 0.960339\n",
      "Iteration 5, loss = 0.14363218\n",
      "Validation score: 0.960339\n",
      "Iteration 6, loss = 0.14161393\n",
      "Validation score: 0.959155\n",
      "Iteration 7, loss = 0.14191518\n",
      "Validation score: 0.960339\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20323987\n",
      "Validation score: 0.957380\n",
      "Iteration 2, loss = 0.15220859\n",
      "Validation score: 0.957380\n",
      "Iteration 3, loss = 0.14685475\n",
      "Validation score: 0.956590\n",
      "Iteration 4, loss = 0.14508715\n",
      "Validation score: 0.958564\n",
      "Iteration 5, loss = 0.14283975\n",
      "Validation score: 0.958761\n",
      "Iteration 6, loss = 0.14315190\n",
      "Validation score: 0.957182\n",
      "Iteration 7, loss = 0.14170273\n",
      "Validation score: 0.956393\n",
      "Iteration 8, loss = 0.14103473\n",
      "Validation score: 0.953039\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27147314\n",
      "Validation score: 0.961531\n",
      "Iteration 2, loss = 0.15733709\n",
      "Validation score: 0.961531\n",
      "Iteration 3, loss = 0.15139725\n",
      "Validation score: 0.962123\n",
      "Iteration 4, loss = 0.14748624\n",
      "Validation score: 0.962320\n",
      "Iteration 5, loss = 0.14617491\n",
      "Validation score: 0.962123\n",
      "Iteration 6, loss = 0.14555613\n",
      "Validation score: 0.962320\n",
      "Iteration 7, loss = 0.14383283\n",
      "Validation score: 0.962123\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18769692\n",
      "Validation score: 0.964286\n",
      "Iteration 2, loss = 0.15321292\n",
      "Validation score: 0.963891\n",
      "Iteration 3, loss = 0.14813500\n",
      "Validation score: 0.964878\n",
      "Iteration 4, loss = 0.14571338\n",
      "Validation score: 0.964483\n",
      "Iteration 5, loss = 0.14297417\n",
      "Validation score: 0.964878\n",
      "Iteration 6, loss = 0.14464307\n",
      "Validation score: 0.964878\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19541417\n",
      "Validation score: 0.964680\n",
      "Iteration 2, loss = 0.15032961\n",
      "Validation score: 0.965272\n",
      "Iteration 3, loss = 0.14828241\n",
      "Validation score: 0.964878\n",
      "Iteration 4, loss = 0.14588806\n",
      "Validation score: 0.964680\n",
      "Iteration 5, loss = 0.14503827\n",
      "Validation score: 0.964680\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18633453\n",
      "Validation score: 0.953442\n",
      "Iteration 2, loss = 0.15454897\n",
      "Validation score: 0.955415\n",
      "Iteration 3, loss = 0.14859570\n",
      "Validation score: 0.955613\n",
      "Iteration 4, loss = 0.14546705\n",
      "Validation score: 0.956007\n",
      "Iteration 5, loss = 0.14444074\n",
      "Validation score: 0.955613\n",
      "Iteration 6, loss = 0.14261327\n",
      "Validation score: 0.955810\n",
      "Iteration 7, loss = 0.14309568\n",
      "Validation score: 0.955613\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18655549\n",
      "Validation score: 0.960537\n",
      "Iteration 2, loss = 0.15642654\n",
      "Validation score: 0.959550\n",
      "Iteration 3, loss = 0.14783159\n",
      "Validation score: 0.960931\n",
      "Iteration 4, loss = 0.14499352\n",
      "Validation score: 0.960931\n",
      "Iteration 5, loss = 0.14268045\n",
      "Validation score: 0.961129\n",
      "Iteration 6, loss = 0.14235667\n",
      "Validation score: 0.960734\n",
      "Iteration 7, loss = 0.14156216\n",
      "Validation score: 0.961129\n",
      "Iteration 8, loss = 0.14097667\n",
      "Validation score: 0.961129\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17099814\n",
      "Validation score: 0.957972\n",
      "Iteration 2, loss = 0.14848285\n",
      "Validation score: 0.959747\n",
      "Iteration 3, loss = 0.14616586\n",
      "Validation score: 0.959945\n",
      "Iteration 4, loss = 0.14302040\n",
      "Validation score: 0.959353\n",
      "Iteration 5, loss = 0.14293406\n",
      "Validation score: 0.959945\n",
      "Iteration 6, loss = 0.14227519\n",
      "Validation score: 0.959155\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16945738\n",
      "Validation score: 0.962320\n",
      "Iteration 2, loss = 0.15219903\n",
      "Validation score: 0.962123\n",
      "Iteration 3, loss = 0.15056931\n",
      "Validation score: 0.962123\n",
      "Iteration 4, loss = 0.14637240\n",
      "Validation score: 0.962320\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25977974\n",
      "Validation score: 0.960931\n",
      "Iteration 2, loss = 0.15417194\n",
      "Validation score: 0.961129\n",
      "Iteration 3, loss = 0.14789436\n",
      "Validation score: 0.960339\n",
      "Iteration 4, loss = 0.14480811\n",
      "Validation score: 0.960734\n",
      "Iteration 5, loss = 0.14357295\n",
      "Validation score: 0.960931\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19417150\n",
      "Validation score: 0.954617\n",
      "Iteration 2, loss = 0.14879048\n",
      "Validation score: 0.953828\n",
      "Iteration 3, loss = 0.14683080\n",
      "Validation score: 0.955012\n",
      "Iteration 4, loss = 0.14258904\n",
      "Validation score: 0.954025\n",
      "Iteration 5, loss = 0.14111559\n",
      "Validation score: 0.953631\n",
      "Iteration 6, loss = 0.14073602\n",
      "Validation score: 0.955012\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17160796\n",
      "Validation score: 0.963504\n",
      "Iteration 2, loss = 0.15540860\n",
      "Validation score: 0.963504\n",
      "Iteration 3, loss = 0.14975403\n",
      "Validation score: 0.964095\n",
      "Iteration 4, loss = 0.14615365\n",
      "Validation score: 0.959953\n",
      "Iteration 5, loss = 0.14644562\n",
      "Validation score: 0.963701\n",
      "Iteration 6, loss = 0.14506720\n",
      "Validation score: 0.964490\n",
      "Iteration 7, loss = 0.14377813\n",
      "Validation score: 0.964293\n",
      "Iteration 8, loss = 0.14355576\n",
      "Validation score: 0.963504\n",
      "Iteration 9, loss = 0.14302793\n",
      "Validation score: 0.964293\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16977483\n",
      "Validation score: 0.958958\n",
      "Iteration 2, loss = 0.16116299\n",
      "Validation score: 0.960142\n",
      "Iteration 3, loss = 0.15630668\n",
      "Validation score: 0.960339\n",
      "Iteration 4, loss = 0.15243476\n",
      "Validation score: 0.959353\n",
      "Iteration 5, loss = 0.15032687\n",
      "Validation score: 0.960931\n",
      "Iteration 6, loss = 0.15283832\n",
      "Validation score: 0.961326\n",
      "Iteration 7, loss = 0.14856908\n",
      "Validation score: 0.960931\n",
      "Iteration 8, loss = 0.14481759\n",
      "Validation score: 0.961129\n",
      "Iteration 9, loss = 0.14408454\n",
      "Validation score: 0.957577\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21207957\n",
      "Validation score: 0.961721\n",
      "Iteration 2, loss = 0.16009572\n",
      "Validation score: 0.963299\n",
      "Iteration 3, loss = 0.15593740\n",
      "Validation score: 0.963496\n",
      "Iteration 4, loss = 0.15837159\n",
      "Validation score: 0.959353\n",
      "Iteration 5, loss = 0.15892782\n",
      "Validation score: 0.964088\n",
      "Iteration 6, loss = 0.15436670\n",
      "Validation score: 0.964088\n",
      "Iteration 7, loss = 0.15238358\n",
      "Validation score: 0.964286\n",
      "Iteration 8, loss = 0.15055283\n",
      "Validation score: 0.964088\n",
      "Iteration 9, loss = 0.14727136\n",
      "Validation score: 0.963891\n",
      "Iteration 10, loss = 0.14716289\n",
      "Validation score: 0.964483\n",
      "Iteration 11, loss = 0.14794885\n",
      "Validation score: 0.962313\n",
      "Iteration 12, loss = 0.14464567\n",
      "Validation score: 0.963891\n",
      "Iteration 13, loss = 0.14841661\n",
      "Validation score: 0.964286\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18648620\n",
      "Validation score: 0.956599\n",
      "Iteration 2, loss = 0.16228476\n",
      "Validation score: 0.958572\n",
      "Iteration 3, loss = 0.15376601\n",
      "Validation score: 0.954823\n",
      "Iteration 4, loss = 0.15229905\n",
      "Validation score: 0.957980\n",
      "Iteration 5, loss = 0.15731938\n",
      "Validation score: 0.958177\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18350225\n",
      "Validation score: 0.957577\n",
      "Iteration 2, loss = 0.15882113\n",
      "Validation score: 0.957380\n",
      "Iteration 3, loss = 0.15398259\n",
      "Validation score: 0.951460\n",
      "Iteration 4, loss = 0.15072963\n",
      "Validation score: 0.957774\n",
      "Iteration 5, loss = 0.15163744\n",
      "Validation score: 0.958169\n",
      "Iteration 6, loss = 0.14984251\n",
      "Validation score: 0.957972\n",
      "Iteration 7, loss = 0.14688661\n",
      "Validation score: 0.956393\n",
      "Iteration 8, loss = 0.14645465\n",
      "Validation score: 0.957577\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20127307\n",
      "Validation score: 0.957972\n",
      "Iteration 2, loss = 0.15347337\n",
      "Validation score: 0.955604\n",
      "Iteration 3, loss = 0.15042793\n",
      "Validation score: 0.958366\n",
      "Iteration 4, loss = 0.15374867\n",
      "Validation score: 0.957972\n",
      "Iteration 5, loss = 0.14929346\n",
      "Validation score: 0.956196\n",
      "Iteration 6, loss = 0.14792389\n",
      "Validation score: 0.957380\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20880446\n",
      "Validation score: 0.946538\n",
      "Iteration 2, loss = 0.16179853\n",
      "Validation score: 0.961728\n",
      "Iteration 3, loss = 0.15752958\n",
      "Validation score: 0.961531\n",
      "Iteration 4, loss = 0.15544651\n",
      "Validation score: 0.960544\n",
      "Iteration 5, loss = 0.15373964\n",
      "Validation score: 0.961925\n",
      "Iteration 6, loss = 0.15019335\n",
      "Validation score: 0.961728\n",
      "Iteration 7, loss = 0.15191337\n",
      "Validation score: 0.961531\n",
      "Iteration 8, loss = 0.15183503\n",
      "Validation score: 0.960742\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17330700\n",
      "Validation score: 0.961523\n",
      "Iteration 2, loss = 0.16207568\n",
      "Validation score: 0.956393\n",
      "Iteration 3, loss = 0.15886924\n",
      "Validation score: 0.961326\n",
      "Iteration 4, loss = 0.15753070\n",
      "Validation score: 0.959945\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22386611\n",
      "Validation score: 0.962904\n",
      "Iteration 2, loss = 0.15986433\n",
      "Validation score: 0.963299\n",
      "Iteration 3, loss = 0.15757579\n",
      "Validation score: 0.964286\n",
      "Iteration 4, loss = 0.15357646\n",
      "Validation score: 0.963299\n",
      "Iteration 5, loss = 0.15071467\n",
      "Validation score: 0.959155\n",
      "Iteration 6, loss = 0.15110486\n",
      "Validation score: 0.964286\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18897854\n",
      "Validation score: 0.960742\n",
      "Iteration 2, loss = 0.16528145\n",
      "Validation score: 0.957191\n",
      "Iteration 3, loss = 0.16058764\n",
      "Validation score: 0.959953\n",
      "Iteration 4, loss = 0.15862445\n",
      "Validation score: 0.960150\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23618838\n",
      "Validation score: 0.955604\n",
      "Iteration 2, loss = 0.15465688\n",
      "Validation score: 0.955209\n",
      "Iteration 3, loss = 0.15352385\n",
      "Validation score: 0.955604\n",
      "Iteration 4, loss = 0.15017203\n",
      "Validation score: 0.955209\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22802844\n",
      "Validation score: 0.957972\n",
      "Iteration 2, loss = 0.15554810\n",
      "Validation score: 0.957577\n",
      "Iteration 3, loss = 0.15449568\n",
      "Validation score: 0.958169\n",
      "Iteration 4, loss = 0.15245692\n",
      "Validation score: 0.957972\n",
      "Iteration 5, loss = 0.14874212\n",
      "Validation score: 0.957972\n",
      "Iteration 6, loss = 0.14731339\n",
      "Validation score: 0.955604\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20227461\n",
      "Validation score: 0.959558\n",
      "Iteration 2, loss = 0.15963346\n",
      "Validation score: 0.960347\n",
      "Iteration 3, loss = 0.15569048\n",
      "Validation score: 0.960544\n",
      "Iteration 4, loss = 0.15171391\n",
      "Validation score: 0.960150\n",
      "Iteration 5, loss = 0.15023016\n",
      "Validation score: 0.960939\n",
      "Iteration 6, loss = 0.15289440\n",
      "Validation score: 0.960544\n",
      "Iteration 7, loss = 0.15043457\n",
      "Validation score: 0.959755\n",
      "Iteration 8, loss = 0.15187526\n",
      "Validation score: 0.960150\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17306886\n",
      "Validation score: 0.959353\n",
      "Iteration 2, loss = 0.16083081\n",
      "Validation score: 0.959155\n",
      "Iteration 3, loss = 0.15616356\n",
      "Validation score: 0.959155\n",
      "Iteration 4, loss = 0.15784129\n",
      "Validation score: 0.958958\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18660922\n",
      "Validation score: 0.954617\n",
      "Iteration 2, loss = 0.15408209\n",
      "Validation score: 0.958564\n",
      "Iteration 3, loss = 0.15818512\n",
      "Validation score: 0.956590\n",
      "Iteration 4, loss = 0.15383398\n",
      "Validation score: 0.958761\n",
      "Iteration 5, loss = 0.14915082\n",
      "Validation score: 0.958564\n",
      "Iteration 6, loss = 0.14829799\n",
      "Validation score: 0.958761\n",
      "Iteration 7, loss = 0.15195234\n",
      "Validation score: 0.958564\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19481245\n",
      "Validation score: 0.960544\n",
      "Iteration 2, loss = 0.16700883\n",
      "Validation score: 0.962517\n",
      "Iteration 3, loss = 0.16855212\n",
      "Validation score: 0.962517\n",
      "Iteration 4, loss = 0.15957186\n",
      "Validation score: 0.962320\n",
      "Iteration 5, loss = 0.15865729\n",
      "Validation score: 0.962517\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25799144\n",
      "Validation score: 0.957182\n",
      "Iteration 2, loss = 0.15261562\n",
      "Validation score: 0.957182\n",
      "Iteration 3, loss = 0.14718247\n",
      "Validation score: 0.957182\n",
      "Iteration 4, loss = 0.14469505\n",
      "Validation score: 0.957182\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23975333\n",
      "Validation score: 0.959747\n",
      "Iteration 2, loss = 0.14737866\n",
      "Validation score: 0.959747\n",
      "Iteration 3, loss = 0.14659425\n",
      "Validation score: 0.959747\n",
      "Iteration 4, loss = 0.14504990\n",
      "Validation score: 0.959747\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26607757\n",
      "Validation score: 0.956993\n",
      "Iteration 2, loss = 0.14997030\n",
      "Validation score: 0.957388\n",
      "Iteration 3, loss = 0.14684981\n",
      "Validation score: 0.957388\n",
      "Iteration 4, loss = 0.14439289\n",
      "Validation score: 0.957388\n",
      "Iteration 5, loss = 0.14428908\n",
      "Validation score: 0.957388\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22029023\n",
      "Validation score: 0.959550\n",
      "Iteration 2, loss = 0.14679939\n",
      "Validation score: 0.959550\n",
      "Iteration 3, loss = 0.14604275\n",
      "Validation score: 0.959550\n",
      "Iteration 4, loss = 0.14596317\n",
      "Validation score: 0.959550\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26434597\n",
      "Validation score: 0.954617\n",
      "Iteration 2, loss = 0.16334488\n",
      "Validation score: 0.954420\n",
      "Iteration 3, loss = 0.14857760\n",
      "Validation score: 0.952052\n",
      "Iteration 4, loss = 0.14401477\n",
      "Validation score: 0.954617\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24754362\n",
      "Validation score: 0.958572\n",
      "Iteration 2, loss = 0.16446632\n",
      "Validation score: 0.961136\n",
      "Iteration 3, loss = 0.15012217\n",
      "Validation score: 0.961728\n",
      "Iteration 4, loss = 0.14793104\n",
      "Validation score: 0.961728\n",
      "Iteration 5, loss = 0.14800240\n",
      "Validation score: 0.961728\n",
      "Iteration 6, loss = 0.14609747\n",
      "Validation score: 0.961728\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24903679\n",
      "Validation score: 0.958366\n",
      "Iteration 2, loss = 0.15205558\n",
      "Validation score: 0.963102\n",
      "Iteration 3, loss = 0.14953079\n",
      "Validation score: 0.963496\n",
      "Iteration 4, loss = 0.14949727\n",
      "Validation score: 0.963496\n",
      "Iteration 5, loss = 0.14732092\n",
      "Validation score: 0.963496\n",
      "Iteration 6, loss = 0.14611044\n",
      "Validation score: 0.963299\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23341394\n",
      "Validation score: 0.960142\n",
      "Iteration 2, loss = 0.14832495\n",
      "Validation score: 0.960142\n",
      "Iteration 3, loss = 0.14894124\n",
      "Validation score: 0.960142\n",
      "Iteration 4, loss = 0.14574772\n",
      "Validation score: 0.960142\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27205273\n",
      "Validation score: 0.959953\n",
      "Iteration 2, loss = 0.15009772\n",
      "Validation score: 0.959953\n",
      "Iteration 3, loss = 0.14730696\n",
      "Validation score: 0.959953\n",
      "Iteration 4, loss = 0.14499841\n",
      "Validation score: 0.959953\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18300697\n",
      "Validation score: 0.959550\n",
      "Iteration 2, loss = 0.14903270\n",
      "Validation score: 0.959550\n",
      "Iteration 3, loss = 0.14728606\n",
      "Validation score: 0.959550\n",
      "Iteration 4, loss = 0.14382888\n",
      "Validation score: 0.959550\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22005832\n",
      "Validation score: 0.959747\n",
      "Iteration 2, loss = 0.15157066\n",
      "Validation score: 0.959747\n",
      "Iteration 3, loss = 0.14814038\n",
      "Validation score: 0.959747\n",
      "Iteration 4, loss = 0.14521265\n",
      "Validation score: 0.959747\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23940156\n",
      "Validation score: 0.964490\n",
      "Iteration 2, loss = 0.15325598\n",
      "Validation score: 0.964490\n",
      "Iteration 3, loss = 0.14869986\n",
      "Validation score: 0.964490\n",
      "Iteration 4, loss = 0.14707778\n",
      "Validation score: 0.964490\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32222592\n",
      "Validation score: 0.946922\n",
      "Iteration 2, loss = 0.15546159\n",
      "Validation score: 0.964680\n",
      "Iteration 3, loss = 0.14947980\n",
      "Validation score: 0.965864\n",
      "Iteration 4, loss = 0.14761702\n",
      "Validation score: 0.965864\n",
      "Iteration 5, loss = 0.14546102\n",
      "Validation score: 0.965864\n",
      "Iteration 6, loss = 0.14509584\n",
      "Validation score: 0.965864\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24694052\n",
      "Validation score: 0.961129\n",
      "Iteration 2, loss = 0.15171314\n",
      "Validation score: 0.960142\n",
      "Iteration 3, loss = 0.14733525\n",
      "Validation score: 0.961129\n",
      "Iteration 4, loss = 0.14622841\n",
      "Validation score: 0.961129\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25741146\n",
      "Validation score: 0.960939\n",
      "Iteration 2, loss = 0.15118652\n",
      "Validation score: 0.960939\n",
      "Iteration 3, loss = 0.14899244\n",
      "Validation score: 0.960939\n",
      "Iteration 4, loss = 0.14665633\n",
      "Validation score: 0.960939\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46371426\n",
      "Validation score: 0.965667\n",
      "Iteration 2, loss = 0.15688056\n",
      "Validation score: 0.965667\n",
      "Iteration 3, loss = 0.15478183\n",
      "Validation score: 0.965667\n",
      "Iteration 4, loss = 0.15438257\n",
      "Validation score: 0.965667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35259722\n",
      "Validation score: 0.960734\n",
      "Iteration 2, loss = 0.17449349\n",
      "Validation score: 0.960734\n",
      "Iteration 3, loss = 0.17394502\n",
      "Validation score: 0.960734\n",
      "Iteration 4, loss = 0.17329247\n",
      "Validation score: 0.960734\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44530709\n",
      "Validation score: 0.960347\n",
      "Iteration 2, loss = 0.17270116\n",
      "Validation score: 0.960347\n",
      "Iteration 3, loss = 0.17212666\n",
      "Validation score: 0.960347\n",
      "Iteration 4, loss = 0.17168625\n",
      "Validation score: 0.960347\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94128012\n",
      "Validation score: 0.958169\n",
      "Iteration 2, loss = 0.18734696\n",
      "Validation score: 0.957972\n",
      "Iteration 3, loss = 0.15782929\n",
      "Validation score: 0.957182\n",
      "Iteration 4, loss = 0.15095812\n",
      "Validation score: 0.957972\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40815156\n",
      "Validation score: 0.961326\n",
      "Iteration 2, loss = 0.15867140\n",
      "Validation score: 0.961326\n",
      "Iteration 3, loss = 0.15414605\n",
      "Validation score: 0.961326\n",
      "Iteration 4, loss = 0.15408899\n",
      "Validation score: 0.961326\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46908216\n",
      "Validation score: 0.956599\n",
      "Iteration 2, loss = 0.15749636\n",
      "Validation score: 0.956599\n",
      "Iteration 3, loss = 0.15567101\n",
      "Validation score: 0.956599\n",
      "Iteration 4, loss = 0.15558960\n",
      "Validation score: 0.956599\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06432614\n",
      "Validation score: 0.960339\n",
      "Iteration 2, loss = 0.16755099\n",
      "Validation score: 0.960537\n",
      "Iteration 3, loss = 0.17424619\n",
      "Validation score: 0.960537\n",
      "Iteration 4, loss = 0.15829780\n",
      "Validation score: 0.960537\n",
      "Iteration 5, loss = 0.15409855\n",
      "Validation score: 0.960537\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41419707\n",
      "Validation score: 0.960931\n",
      "Iteration 2, loss = 0.17537246\n",
      "Validation score: 0.960931\n",
      "Iteration 3, loss = 0.17410594\n",
      "Validation score: 0.960931\n",
      "Iteration 4, loss = 0.17401141\n",
      "Validation score: 0.960931\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39110273\n",
      "Validation score: 0.955613\n",
      "Iteration 2, loss = 0.15626696\n",
      "Validation score: 0.955613\n",
      "Iteration 3, loss = 0.15674194\n",
      "Validation score: 0.947327\n",
      "Iteration 4, loss = 0.15748974\n",
      "Validation score: 0.955613\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52792307\n",
      "Validation score: 0.959747\n",
      "Iteration 2, loss = 0.17211318\n",
      "Validation score: 0.959747\n",
      "Iteration 3, loss = 0.17161223\n",
      "Validation score: 0.959747\n",
      "Iteration 4, loss = 0.17634518\n",
      "Validation score: 0.959747\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40775037\n",
      "Validation score: 0.962707\n",
      "Iteration 2, loss = 0.17402030\n",
      "Validation score: 0.962707\n",
      "Iteration 3, loss = 0.17313720\n",
      "Validation score: 0.962707\n",
      "Iteration 4, loss = 0.17285331\n",
      "Validation score: 0.962707\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09710157\n",
      "Validation score: 0.961531\n",
      "Iteration 2, loss = 0.16647114\n",
      "Validation score: 0.961334\n",
      "Iteration 3, loss = 0.15546674\n",
      "Validation score: 0.961334\n",
      "Iteration 4, loss = 0.15102236\n",
      "Validation score: 0.961334\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51005516\n",
      "Validation score: 0.959550\n",
      "Iteration 2, loss = 0.17233491\n",
      "Validation score: 0.959550\n",
      "Iteration 3, loss = 0.17213402\n",
      "Validation score: 0.959550\n",
      "Iteration 4, loss = 0.17164581\n",
      "Validation score: 0.959550\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.26735954\n",
      "Validation score: 0.870363\n",
      "Iteration 2, loss = 0.29701564\n",
      "Validation score: 0.959550\n",
      "Iteration 3, loss = 0.16917848\n",
      "Validation score: 0.959550\n",
      "Iteration 4, loss = 0.15607028\n",
      "Validation score: 0.959550\n",
      "Iteration 5, loss = 0.15383107\n",
      "Validation score: 0.959550\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01720290\n",
      "Validation score: 0.951470\n",
      "Iteration 2, loss = 0.16736515\n",
      "Validation score: 0.959361\n",
      "Iteration 3, loss = 0.15444552\n",
      "Validation score: 0.959361\n",
      "Iteration 4, loss = 0.15160012\n",
      "Validation score: 0.959361\n",
      "Iteration 5, loss = 0.15059692\n",
      "Validation score: 0.959361\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21871835\n",
      "Validation score: 0.960734\n",
      "Iteration 2, loss = 0.15605136\n",
      "Validation score: 0.961523\n",
      "Iteration 3, loss = 0.14974476\n",
      "Validation score: 0.960537\n",
      "Iteration 4, loss = 0.14857475\n",
      "Validation score: 0.960931\n",
      "Iteration 5, loss = 0.14635114\n",
      "Validation score: 0.962707\n",
      "Iteration 6, loss = 0.14644069\n",
      "Validation score: 0.962510\n",
      "Iteration 7, loss = 0.14434553\n",
      "Validation score: 0.962510\n",
      "Iteration 8, loss = 0.14338440\n",
      "Validation score: 0.962707\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20692410\n",
      "Validation score: 0.958169\n",
      "Iteration 2, loss = 0.15313928\n",
      "Validation score: 0.957972\n",
      "Iteration 3, loss = 0.14784203\n",
      "Validation score: 0.958366\n",
      "Iteration 4, loss = 0.14534574\n",
      "Validation score: 0.958169\n",
      "Iteration 5, loss = 0.14433256\n",
      "Validation score: 0.958564\n",
      "Iteration 6, loss = 0.14262035\n",
      "Validation score: 0.953433\n",
      "Iteration 7, loss = 0.14320101\n",
      "Validation score: 0.957972\n",
      "Iteration 8, loss = 0.14131374\n",
      "Validation score: 0.958366\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23839794\n",
      "Validation score: 0.958769\n",
      "Iteration 2, loss = 0.15584550\n",
      "Validation score: 0.958769\n",
      "Iteration 3, loss = 0.15050255\n",
      "Validation score: 0.958769\n",
      "Iteration 4, loss = 0.14792184\n",
      "Validation score: 0.957980\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27961881\n",
      "Validation score: 0.959945\n",
      "Iteration 2, loss = 0.15248215\n",
      "Validation score: 0.960734\n",
      "Iteration 3, loss = 0.14798746\n",
      "Validation score: 0.959747\n",
      "Iteration 4, loss = 0.14558332\n",
      "Validation score: 0.960339\n",
      "Iteration 5, loss = 0.14460622\n",
      "Validation score: 0.959747\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18293466\n",
      "Validation score: 0.962510\n",
      "Iteration 2, loss = 0.14896308\n",
      "Validation score: 0.962904\n",
      "Iteration 3, loss = 0.14662651\n",
      "Validation score: 0.963299\n",
      "Iteration 4, loss = 0.14577944\n",
      "Validation score: 0.963102\n",
      "Iteration 5, loss = 0.14376157\n",
      "Validation score: 0.962904\n",
      "Iteration 6, loss = 0.14282808\n",
      "Validation score: 0.963299\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19103368\n",
      "Validation score: 0.958374\n",
      "Iteration 2, loss = 0.15505927\n",
      "Validation score: 0.960939\n",
      "Iteration 3, loss = 0.14901735\n",
      "Validation score: 0.961334\n",
      "Iteration 4, loss = 0.14740685\n",
      "Validation score: 0.961334\n",
      "Iteration 5, loss = 0.14545248\n",
      "Validation score: 0.961334\n",
      "Iteration 6, loss = 0.14485047\n",
      "Validation score: 0.961531\n",
      "Iteration 7, loss = 0.14424652\n",
      "Validation score: 0.961334\n",
      "Iteration 8, loss = 0.14426749\n",
      "Validation score: 0.961136\n",
      "Iteration 9, loss = 0.14261478\n",
      "Validation score: 0.961728\n",
      "Iteration 10, loss = 0.14193797\n",
      "Validation score: 0.961728\n",
      "Iteration 11, loss = 0.14346967\n",
      "Validation score: 0.961531\n",
      "Iteration 12, loss = 0.14320859\n",
      "Validation score: 0.961728\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20324774\n",
      "Validation score: 0.959155\n",
      "Iteration 2, loss = 0.15317189\n",
      "Validation score: 0.959155\n",
      "Iteration 3, loss = 0.15002581\n",
      "Validation score: 0.959155\n",
      "Iteration 4, loss = 0.14560695\n",
      "Validation score: 0.959353\n",
      "Iteration 5, loss = 0.14330330\n",
      "Validation score: 0.958958\n",
      "Iteration 6, loss = 0.14196215\n",
      "Validation score: 0.959550\n",
      "Iteration 7, loss = 0.14157263\n",
      "Validation score: 0.958761\n",
      "Iteration 8, loss = 0.14289145\n",
      "Validation score: 0.959353\n",
      "Iteration 9, loss = 0.14060081\n",
      "Validation score: 0.959353\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18030912\n",
      "Validation score: 0.959945\n",
      "Iteration 2, loss = 0.15113551\n",
      "Validation score: 0.959945\n",
      "Iteration 3, loss = 0.14778350\n",
      "Validation score: 0.959945\n",
      "Iteration 4, loss = 0.14558317\n",
      "Validation score: 0.959945\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17277510\n",
      "Validation score: 0.955613\n",
      "Iteration 2, loss = 0.14991854\n",
      "Validation score: 0.955021\n",
      "Iteration 3, loss = 0.14721379\n",
      "Validation score: 0.956402\n",
      "Iteration 4, loss = 0.14507660\n",
      "Validation score: 0.956599\n",
      "Iteration 5, loss = 0.14454547\n",
      "Validation score: 0.956402\n",
      "Iteration 6, loss = 0.14269313\n",
      "Validation score: 0.956599\n",
      "Iteration 7, loss = 0.14236663\n",
      "Validation score: 0.956599\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17564094\n",
      "Validation score: 0.958169\n",
      "Iteration 2, loss = 0.14869404\n",
      "Validation score: 0.958366\n",
      "Iteration 3, loss = 0.14458946\n",
      "Validation score: 0.957972\n",
      "Iteration 4, loss = 0.14264844\n",
      "Validation score: 0.957972\n",
      "Iteration 5, loss = 0.14243308\n",
      "Validation score: 0.958169\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31499158\n",
      "Validation score: 0.956590\n",
      "Iteration 2, loss = 0.15816548\n",
      "Validation score: 0.958761\n",
      "Iteration 3, loss = 0.15132240\n",
      "Validation score: 0.957182\n",
      "Iteration 4, loss = 0.14852708\n",
      "Validation score: 0.958366\n",
      "Iteration 5, loss = 0.14615531\n",
      "Validation score: 0.958564\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22807486\n",
      "Validation score: 0.960742\n",
      "Iteration 2, loss = 0.15704209\n",
      "Validation score: 0.961136\n",
      "Iteration 3, loss = 0.15206862\n",
      "Validation score: 0.960939\n",
      "Iteration 4, loss = 0.14963801\n",
      "Validation score: 0.961136\n",
      "Iteration 5, loss = 0.14718710\n",
      "Validation score: 0.961334\n",
      "Iteration 6, loss = 0.14621834\n",
      "Validation score: 0.961334\n",
      "Iteration 7, loss = 0.14438873\n",
      "Validation score: 0.961334\n",
      "Iteration 8, loss = 0.14381640\n",
      "Validation score: 0.961334\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17985815\n",
      "Validation score: 0.954617\n",
      "Iteration 2, loss = 0.15046308\n",
      "Validation score: 0.954617\n",
      "Iteration 3, loss = 0.14667519\n",
      "Validation score: 0.954815\n",
      "Iteration 4, loss = 0.14538056\n",
      "Validation score: 0.955406\n",
      "Iteration 5, loss = 0.14522848\n",
      "Validation score: 0.955801\n",
      "Iteration 6, loss = 0.14259501\n",
      "Validation score: 0.954617\n",
      "Iteration 7, loss = 0.14144082\n",
      "Validation score: 0.953828\n",
      "Iteration 8, loss = 0.14058186\n",
      "Validation score: 0.954617\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51604333\n",
      "Validation score: 0.959353\n",
      "Iteration 2, loss = 0.16418846\n",
      "Validation score: 0.960142\n",
      "Iteration 3, loss = 0.15848867\n",
      "Validation score: 0.960142\n",
      "Iteration 4, loss = 0.15148481\n",
      "Validation score: 0.960734\n",
      "Iteration 5, loss = 0.14824301\n",
      "Validation score: 0.960734\n",
      "Iteration 6, loss = 0.14422106\n",
      "Validation score: 0.960734\n",
      "Iteration 7, loss = 0.14274602\n",
      "Validation score: 0.960931\n",
      "Iteration 8, loss = 0.14230520\n",
      "Validation score: 0.961326\n",
      "Iteration 9, loss = 0.14169841\n",
      "Validation score: 0.960734\n",
      "Iteration 10, loss = 0.14176324\n",
      "Validation score: 0.961326\n",
      "Iteration 11, loss = 0.14029539\n",
      "Validation score: 0.961129\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20384924\n",
      "Validation score: 0.959755\n",
      "Iteration 2, loss = 0.15413731\n",
      "Validation score: 0.961334\n",
      "Iteration 3, loss = 0.15045944\n",
      "Validation score: 0.961334\n",
      "Iteration 4, loss = 0.14895383\n",
      "Validation score: 0.961728\n",
      "Iteration 5, loss = 0.14635728\n",
      "Validation score: 0.960544\n",
      "Iteration 6, loss = 0.14664636\n",
      "Validation score: 0.961728\n",
      "Iteration 7, loss = 0.14480856\n",
      "Validation score: 0.961728\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19988025\n",
      "Validation score: 0.961129\n",
      "Iteration 2, loss = 0.15799729\n",
      "Validation score: 0.961523\n",
      "Iteration 3, loss = 0.15497522\n",
      "Validation score: 0.961523\n",
      "Iteration 4, loss = 0.15388603\n",
      "Validation score: 0.960734\n",
      "Iteration 5, loss = 0.15604132\n",
      "Validation score: 0.961523\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19380829\n",
      "Validation score: 0.961721\n",
      "Iteration 2, loss = 0.16240580\n",
      "Validation score: 0.952447\n",
      "Iteration 3, loss = 0.15850696\n",
      "Validation score: 0.963102\n",
      "Iteration 4, loss = 0.15380395\n",
      "Validation score: 0.963102\n",
      "Iteration 5, loss = 0.15236051\n",
      "Validation score: 0.963102\n",
      "Iteration 6, loss = 0.15009765\n",
      "Validation score: 0.960339\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19786129\n",
      "Validation score: 0.956599\n",
      "Iteration 2, loss = 0.16379668\n",
      "Validation score: 0.960742\n",
      "Iteration 3, loss = 0.15638151\n",
      "Validation score: 0.961136\n",
      "Iteration 4, loss = 0.16082260\n",
      "Validation score: 0.960939\n",
      "Iteration 5, loss = 0.15463450\n",
      "Validation score: 0.961334\n",
      "Iteration 6, loss = 0.15213689\n",
      "Validation score: 0.961334\n",
      "Iteration 7, loss = 0.15012912\n",
      "Validation score: 0.961136\n",
      "Iteration 8, loss = 0.15046344\n",
      "Validation score: 0.961334\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22689330\n",
      "Validation score: 0.959155\n",
      "Iteration 2, loss = 0.15485577\n",
      "Validation score: 0.953631\n",
      "Iteration 3, loss = 0.15335911\n",
      "Validation score: 0.960537\n",
      "Iteration 4, loss = 0.15208472\n",
      "Validation score: 0.961326\n",
      "Iteration 5, loss = 0.15125700\n",
      "Validation score: 0.961129\n",
      "Iteration 6, loss = 0.15151616\n",
      "Validation score: 0.961129\n",
      "Iteration 7, loss = 0.14606276\n",
      "Validation score: 0.961129\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17380756\n",
      "Validation score: 0.957972\n",
      "Iteration 2, loss = 0.15828658\n",
      "Validation score: 0.958169\n",
      "Iteration 3, loss = 0.15386488\n",
      "Validation score: 0.957577\n",
      "Iteration 4, loss = 0.15399774\n",
      "Validation score: 0.958169\n",
      "Iteration 5, loss = 0.15529513\n",
      "Validation score: 0.957380\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21174030\n",
      "Validation score: 0.961728\n",
      "Iteration 2, loss = 0.15838052\n",
      "Validation score: 0.958572\n",
      "Iteration 3, loss = 0.15295297\n",
      "Validation score: 0.962320\n",
      "Iteration 4, loss = 0.15269365\n",
      "Validation score: 0.961136\n",
      "Iteration 5, loss = 0.14993455\n",
      "Validation score: 0.961925\n",
      "Iteration 6, loss = 0.15725611\n",
      "Validation score: 0.962517\n",
      "Iteration 7, loss = 0.14826536\n",
      "Validation score: 0.962320\n",
      "Iteration 8, loss = 0.15370126\n",
      "Validation score: 0.961531\n",
      "Iteration 9, loss = 0.14636688\n",
      "Validation score: 0.962320\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17734073\n",
      "Validation score: 0.956196\n",
      "Iteration 2, loss = 0.15874753\n",
      "Validation score: 0.935675\n",
      "Iteration 3, loss = 0.16053040\n",
      "Validation score: 0.956196\n",
      "Iteration 4, loss = 0.15079139\n",
      "Validation score: 0.955406\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18359452\n",
      "Validation score: 0.958564\n",
      "Iteration 2, loss = 0.15769356\n",
      "Validation score: 0.958366\n",
      "Iteration 3, loss = 0.15460478\n",
      "Validation score: 0.956590\n",
      "Iteration 4, loss = 0.14978781\n",
      "Validation score: 0.958564\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24079847\n",
      "Validation score: 0.959953\n",
      "Iteration 2, loss = 0.16101171\n",
      "Validation score: 0.962517\n",
      "Iteration 3, loss = 0.16105864\n",
      "Validation score: 0.962123\n",
      "Iteration 4, loss = 0.15833937\n",
      "Validation score: 0.956007\n",
      "Iteration 5, loss = 0.15940675\n",
      "Validation score: 0.963109\n",
      "Iteration 6, loss = 0.15677462\n",
      "Validation score: 0.962320\n",
      "Iteration 7, loss = 0.15519354\n",
      "Validation score: 0.948511\n",
      "Iteration 8, loss = 0.15453356\n",
      "Validation score: 0.962320\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17634149\n",
      "Validation score: 0.951855\n",
      "Iteration 2, loss = 0.16938172\n",
      "Validation score: 0.962707\n",
      "Iteration 3, loss = 0.16038425\n",
      "Validation score: 0.962707\n",
      "Iteration 4, loss = 0.15489153\n",
      "Validation score: 0.962904\n",
      "Iteration 5, loss = 0.15560763\n",
      "Validation score: 0.963496\n",
      "Iteration 6, loss = 0.15059095\n",
      "Validation score: 0.960537\n",
      "Iteration 7, loss = 0.15280653\n",
      "Validation score: 0.963496\n",
      "Iteration 8, loss = 0.14652339\n",
      "Validation score: 0.956196\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19735140\n",
      "Validation score: 0.961523\n",
      "Iteration 2, loss = 0.15738569\n",
      "Validation score: 0.944159\n",
      "Iteration 3, loss = 0.16486332\n",
      "Validation score: 0.961326\n",
      "Iteration 4, loss = 0.16041112\n",
      "Validation score: 0.961326\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18287058\n",
      "Validation score: 0.957783\n",
      "Iteration 2, loss = 0.16536321\n",
      "Validation score: 0.957585\n",
      "Iteration 3, loss = 0.15609095\n",
      "Validation score: 0.954429\n",
      "Iteration 4, loss = 0.15681658\n",
      "Validation score: 0.957585\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16999957\n",
      "Validation score: 0.961129\n",
      "Iteration 2, loss = 0.15422637\n",
      "Validation score: 0.961721\n",
      "Iteration 3, loss = 0.15065104\n",
      "Validation score: 0.961523\n",
      "Iteration 4, loss = 0.14728109\n",
      "Validation score: 0.961721\n",
      "Iteration 5, loss = 0.14709266\n",
      "Validation score: 0.961523\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20487855\n",
      "Validation score: 0.961721\n",
      "Iteration 2, loss = 0.15772304\n",
      "Validation score: 0.960537\n",
      "Iteration 3, loss = 0.15894899\n",
      "Validation score: 0.961326\n",
      "Iteration 4, loss = 0.15327315\n",
      "Validation score: 0.957380\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19875937\n",
      "Validation score: 0.964293\n",
      "Iteration 2, loss = 0.16305851\n",
      "Validation score: 0.959164\n",
      "Iteration 3, loss = 0.16339374\n",
      "Validation score: 0.960347\n",
      "Iteration 4, loss = 0.15902312\n",
      "Validation score: 0.963504\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24186613\n",
      "Validation score: 0.953433\n",
      "Iteration 2, loss = 0.16255150\n",
      "Validation score: 0.947908\n",
      "Iteration 3, loss = 0.15315898\n",
      "Validation score: 0.957577\n",
      "Iteration 4, loss = 0.14564940\n",
      "Validation score: 0.957577\n",
      "Iteration 5, loss = 0.14440302\n",
      "Validation score: 0.957577\n",
      "Iteration 6, loss = 0.14414839\n",
      "Validation score: 0.957577\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22577405\n",
      "Validation score: 0.954815\n",
      "Iteration 2, loss = 0.15023789\n",
      "Validation score: 0.958761\n",
      "Iteration 3, loss = 0.14771574\n",
      "Validation score: 0.958761\n",
      "Iteration 4, loss = 0.14458876\n",
      "Validation score: 0.958761\n",
      "Iteration 5, loss = 0.14431503\n",
      "Validation score: 0.958761\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26824662\n",
      "Validation score: 0.963504\n",
      "Iteration 2, loss = 0.16265502\n",
      "Validation score: 0.963898\n",
      "Iteration 3, loss = 0.15066787\n",
      "Validation score: 0.963898\n",
      "Iteration 4, loss = 0.14823033\n",
      "Validation score: 0.963898\n",
      "Iteration 5, loss = 0.14785175\n",
      "Validation score: 0.962912\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24718472\n",
      "Validation score: 0.958958\n",
      "Iteration 2, loss = 0.15634467\n",
      "Validation score: 0.958958\n",
      "Iteration 3, loss = 0.15129841\n",
      "Validation score: 0.957972\n",
      "Iteration 4, loss = 0.14882115\n",
      "Validation score: 0.958958\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22599262\n",
      "Validation score: 0.964088\n",
      "Iteration 2, loss = 0.15290262\n",
      "Validation score: 0.964483\n",
      "Iteration 3, loss = 0.14883136\n",
      "Validation score: 0.964483\n",
      "Iteration 4, loss = 0.14626481\n",
      "Validation score: 0.964483\n",
      "Iteration 5, loss = 0.14903540\n",
      "Validation score: 0.964483\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24605758\n",
      "Validation score: 0.962517\n",
      "Iteration 2, loss = 0.15386590\n",
      "Validation score: 0.962517\n",
      "Iteration 3, loss = 0.15106395\n",
      "Validation score: 0.962517\n",
      "Iteration 4, loss = 0.15018433\n",
      "Validation score: 0.962320\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24776084\n",
      "Validation score: 0.958366\n",
      "Iteration 2, loss = 0.15311356\n",
      "Validation score: 0.957380\n",
      "Iteration 3, loss = 0.14797314\n",
      "Validation score: 0.958761\n",
      "Iteration 4, loss = 0.14683497\n",
      "Validation score: 0.958761\n",
      "Iteration 5, loss = 0.14531158\n",
      "Validation score: 0.958761\n",
      "Iteration 6, loss = 0.14615404\n",
      "Validation score: 0.958761\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25503327\n",
      "Validation score: 0.955801\n",
      "Iteration 2, loss = 0.15052488\n",
      "Validation score: 0.955604\n",
      "Iteration 3, loss = 0.14662830\n",
      "Validation score: 0.955604\n",
      "Iteration 4, loss = 0.14508833\n",
      "Validation score: 0.955801\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23638603\n",
      "Validation score: 0.961925\n",
      "Iteration 2, loss = 0.15139491\n",
      "Validation score: 0.962123\n",
      "Iteration 3, loss = 0.14866353\n",
      "Validation score: 0.962123\n",
      "Iteration 4, loss = 0.14781218\n",
      "Validation score: 0.962123\n",
      "Iteration 5, loss = 0.14608959\n",
      "Validation score: 0.962123\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26733756\n",
      "Validation score: 0.955998\n",
      "Iteration 2, loss = 0.15019286\n",
      "Validation score: 0.957182\n",
      "Iteration 3, loss = 0.14748277\n",
      "Validation score: 0.957182\n",
      "Iteration 4, loss = 0.14770229\n",
      "Validation score: 0.957182\n",
      "Iteration 5, loss = 0.14429152\n",
      "Validation score: 0.957182\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19088457\n",
      "Validation score: 0.960931\n",
      "Iteration 2, loss = 0.14848847\n",
      "Validation score: 0.960931\n",
      "Iteration 3, loss = 0.14613385\n",
      "Validation score: 0.960931\n",
      "Iteration 4, loss = 0.14514655\n",
      "Validation score: 0.960931\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26740972\n",
      "Validation score: 0.958769\n",
      "Iteration 2, loss = 0.15156777\n",
      "Validation score: 0.958769\n",
      "Iteration 3, loss = 0.15575280\n",
      "Validation score: 0.958769\n",
      "Iteration 4, loss = 0.14670763\n",
      "Validation score: 0.958769\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "\n",
    "parameters = {'hidden_layer_sizes': [(100,), \n",
    "                                     (50,), \n",
    "                                     (300,), \n",
    "                                     (200,100,50), \n",
    "                                     (200,50,10), \n",
    "                                     (200,100,10), \n",
    "                                     (100,50,10), \n",
    "                                     (100,10), \n",
    "                                     (50,10), \n",
    "                                     (300,200,100,50), \n",
    "                                     (300,200,100,50,10)],\n",
    "              'activation': ['relu', 'tanh'],\n",
    "              'learning_rate': ['constant', 'adaptive'],\n",
    "              'learning_rate_init': [0.0001, 0.001, 0.01, 0.1],\n",
    "              'max_iter': [50, 100, 200, 300, 500],\n",
    "              'early_stopping': [True, False], \n",
    "              'verbose': [True]}\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring='roc_auc')\n",
    "grid_obj = grid_obj.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label='yes')\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'C': [0.01, 0.1, 1, 10, 100],\n",
    "             'degree': [1,2,3,4,5],\n",
    "             'kernel':  ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "clf = SVM()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label='yes')\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Undersampling\n",
    "- This dataset is highly unbalanced, so using the new dataset without noise, I made an undersampling as follows: first, I splitted the dataset in the instances which have class label 0 and the instances which have class label 1. Later, I splitted the instances with 0 class into (number_of_class0/number_of_class1) partitions (24 partitions) and I joined each partition with the partition of instances of 1 class label made before. Finally, I ran xgboost on each new balanced partition and I made an average with the predictions of the 24 models.\n",
    "- Add this undersampling method to cross val function, with partameter to toggle on/off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot = {c: list(train[c].unique()) for c in train[features].select_dtypes('int').columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot([len(i) for i in one_hot.values()], kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot([len(i) for i in one_hot.values() if len(i)<50], kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def OHE_by_unique(df, one_hot, limit):    \n",
    "    #ONE-HOT encode features with more than 2 and less than 'limit' unique values\n",
    "    for c in one_hot:\n",
    "        if len(one_hot[c])>2 and len(one_hot[c]) < limit:\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "\n",
    "OHE_by_unique(train, one_hot, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Scale remainders of OHE (unique < limit)\n",
    "transform unique?limit to float at begining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extra Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Classifiers as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature correlation and importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Scale new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross Validation Test\n",
    "- comparar:\n",
    "    - CrossValidation inside and outside (parameter to toggle between True and False)\n",
    "    - faz diferença?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_val_model(X,y, model, n_splits=5, scoring='roc_auc', undersampling_folds=True):\n",
    "    if outside_cv:\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1).split(X, y))\n",
    "        score = 0\n",
    "\n",
    "        for j, (train_idx, test_idx) in enumerate(folds):\n",
    "            X_train = X[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            X_holdout = X[test_idx]\n",
    "            y_holdout = y[test_idx]\n",
    "\n",
    "            print (\"Fit %s fold %d\" % (str(model).split('(')[0], j+1))\n",
    "            model.fit(X_train, y_train)\n",
    "            cross_score = cross_val_score(model, X_holdout, y_holdout, cv=3, scoring=scoring)\n",
    "            score+= cross_score.mean()\n",
    "        score = score/n_splits\n",
    "    else:\n",
    "        score = cross_val_score(model, X_holdout, y_holdout, cv=3, scoring=scoring).mean()\n",
    "        \n",
    "        # check if sklearn cross_val uses stratified kfold\n",
    "        \n",
    "    print(\"    cross_score: %.5f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Stacking / Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ideas\n",
    "- Add noise to target\n",
    "- Add noise to target after folding, and train a regressor.\n",
    "- Replicate datapoints with TARGET=1. \n",
    "- Replicate datapoints with TARGET=1. Add noise.\n",
    "- Unbalanced features: check if they give info about target. Try to combine them. Remove excess.\n",
    "- Remaining features: check if they give info using FacetGrid. Apply transformations and check again. Trying to kernel trick.\n",
    "- Simplyfy cross-validation using only sklearn's cross_val_score.\n",
    "- Remove datapoints that have outliers if target=0\n",
    "- Submit using Kaggle api. import kaggle\n",
    "- plotar distribuicao dos scores finais da competicao\n",
    "- mostrar minha posicao comparada as outras notas sem rankear. estara mto mais proxima do primeiro lugar do que do benchmark\n",
    "- ao terminar o projeto, responder email antigo da cristina junqueira\n",
    "- Review previous projects' and live's notebooks ot see if there's something to be added\n",
    "- Test the baseline with unprocessed data\n",
    "- Test the baseline with unscaled data\n",
    "- GridSearch again at the end, after feature engineering\n",
    "- SVM on low importance features.\n",
    "- drop duplicated columns. `train.T.drop_duplicates().T`\n",
    "- fit scaler to whole data (train + test)\n",
    "- Try more base models with final data\n",
    "- Train models only on misclassified datapoints to serve as features for the final model\n",
    "- Link MLP http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "- log scaler\n",
    "- OneHotEncode'em all\n",
    "- Binary feature for each feature on the dataset. `1 if feature == feature.mode() else 0`\n",
    "- Add column that sum zeros on row. `train['zeros'] = (train==0).sum(axis=1).astype(float)`\n",
    "- Add column that sums values in row. \n",
    "- PCA and other dimensionality reduction on low importance features.\n",
    "- ONE-HOT encode features with more than 2 and less than 'limit' unique values\n",
    "- Test importance of new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sklearn Pipeline (Optional)\n",
    "### Scalers\n",
    "### Cross Validation Folds\n",
    "### Models\n",
    "### Grid Search\n",
    "\n",
    "- https://stackoverflow.com/questions/49262900/scaling-data-using-pipelines-in-scikit-learn-standardscaler-vs-robustscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(train.drop('TARGET', axis=1))\n",
    "y = np.array(train['TARGET'])\n",
    "\n",
    "xgb_classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = np.array(test)\n",
    "y_pred = xgb_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['target'] = y_pred[:,1]\n",
    "sub.to_csv('xgb29.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualization Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from IPython.display import set_matplotlib_formats\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# sns.set(color_codes=True, palette=sns.palplot(sns.cubehelix_palette(8)))\n",
    "# sns.set_palette(\"cubehelix_palette\")\n",
    "%matplotlib inline\n",
    "# set_matplotlib_formats('pdf', 'png')\n",
    "rc={'savefig.dpi': 75,    'figure.autolayout': False,    'figure.figsize': [8, 6],    'axes.labelsize': 18,    'axes.titlesize': 18,    'font.size': 18,    'lines.linewidth': 2.0,    'lines.markersize': 8,    'legend.fontsize': 16,    'xtick.labelsize': 16,    'ytick.labelsize': 16,    }\n",
    "# sns.set(style='dark')\n",
    "sns.set(style='dark',rc=rc)\n",
    "# default_color = '#56B4E9'\n",
    "colormap = plt.cm.cool\n",
    "# Import supplementary visualizations code visuals.py\n",
    "# import visuals as vs\n",
    "# Setting working directory\n",
    "# path = '../data/raw/'\n",
    "sns.set_palette(sns.cubehelix_palette(50))\n",
    "sns.set_palette('GnBu_d')\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.countplot(meta.d_type);\n",
    "sns.countplot(train.dtypes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.title('Some variables have several features related o it:')\n",
    "sns.countplot(meta.variable);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.title('Some features may appear for lots of variables. Others are variable specific.')\n",
    "meta.attribute.value_counts().head(15).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.title('Most integer features have less than 25 unique values.')\n",
    "plt.xticks(size=8, rotation=30)\n",
    "sns.countplot(meta.loc[(meta.d_type=='integer') & meta.keep].n_unique);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.xticks(size=8, rotation=90)\n",
    "sns.countplot(meta.loc[(meta.d_type=='float') & meta.keep].n_unique);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for c in train[meta.loc[(meta.d_type=='float') & meta.keep]['name']]:\n",
    "#     sns.kdeplot(train[c].apply(lambda x: 0.0001 if x==0 else x).apply(np.log), legend=False)\n",
    "#     sns.kdeplot(train[c].apply(np.log), legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for c in train.select_dtypes('float'):\n",
    "    sns.distplot(train[c]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(meta.d_type);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preparation / Preprocessing / Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "remove data which meta[keep=false]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[col].max().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train['delta_num_compra_var44_1y3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for c in col2:\n",
    "    sns.distplot(train[c].apply(lambda x: np.log(x+1.23456789)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# adiantando \n",
    "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n",
    "\n",
    "def OHE_by_unique(df, one_hot, limit):\n",
    "    \n",
    "    #ONE-HOT encode features with more than 2 and less than 'limit' unique values\n",
    "    for c in one_hot:\n",
    "        if len(one_hot[c])>2 and len(one_hot[c]) < limit:\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "OHE_by_unique(train, one_hot, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "\n",
    "def identify_constant_features(dataframe):\n",
    "    count_uniques = dataframe.apply(lambda x: len(x.unique()))\n",
    "    constants = count_uniques[count_uniques == 1].index.tolist()\n",
    "    return constants\n",
    "\n",
    "constant_features_train = set(identify_constant_features(train_dataset))\n",
    "\n",
    "print('There were {} constant features in TRAIN dataset.'.format(\n",
    "        len(constant_features_train)))\n",
    "\n",
    "# Drop the constant features\n",
    "train_dataset.drop(constant_features_train, inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'All zeros benchmark has ROC_AUC score of {}'.format(roc_auc_score(train.TARGET, np.zeros_like(train.TARGET)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_val_model(X,y, model, n_splits=5):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    folds = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1).split(X, y))\n",
    "    score = 0\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        X_train = X[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        X_holdout = X[test_idx]\n",
    "        y_holdout = y[test_idx]\n",
    "\n",
    "        print (\"Fit %s fold %d\" % (str(model).split('(')[0], j+1))\n",
    "        model.fit(X_train, y_train)\n",
    "        cross_score = cross_val_score(model, X_holdout, y_holdout, cv=3, scoring='roc_auc')\n",
    "        print(cross_score.mean())\n",
    "        score+= cross_score.mean()\n",
    "    score = score/n_splits\n",
    "    print(\"    cross_score: %.5f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "clf_A = GaussianNB()\n",
    "clf_B = DecisionTreeClassifier()\n",
    "# clf_C = SVM()\n",
    "clf_D = LogisticRegression()\n",
    "clf_E = RandomForestClassifier()\n",
    "\n",
    "X = train.drop('TARGET', axis=1)\n",
    "X = train.drop(['TARGET'], axis=1)\n",
    "y = train['TARGET']\n",
    "\n",
    "# u'strange_in_TARGET' should be droped\n",
    "\n",
    "# # TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "# for clf in [clf_A, clf_B, clf_C]:\n",
    "#     for X_set, y_set in [(X_train_100, y_train_100), (X_train_200, y_train_200), (X_train_300, y_train_300)]:\n",
    "#         train_predict(clf, X_set, y_set, X_test, y_test)\n",
    "#         print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit GaussianNB fold 1\n",
      "0.5383638500144288\n",
      "Fit GaussianNB fold 2\n",
      "0.5232403335529742\n",
      "Fit GaussianNB fold 3\n",
      "0.5361399576207746\n",
      "Fit GaussianNB fold 4\n",
      "0.5356069617075271\n",
      "Fit GaussianNB fold 5\n",
      "0.5317914890445244\n",
      "    cross_score: 0.53303\n",
      "Fit DecisionTreeClassifier fold 1\n",
      "0.5581280479549477\n",
      "Fit DecisionTreeClassifier fold 2\n",
      "0.5659181928915068\n",
      "Fit DecisionTreeClassifier fold 3\n",
      "0.5559330271160843\n",
      "Fit DecisionTreeClassifier fold 4\n",
      "0.5561279287486526\n",
      "Fit DecisionTreeClassifier fold 5\n",
      "0.5616064872781559\n",
      "    cross_score: 0.55954\n",
      "Fit LogisticRegression fold 1\n",
      "0.37906670294236494\n",
      "Fit LogisticRegression fold 2\n",
      "0.38076612457038816\n",
      "Fit LogisticRegression fold 3\n",
      "0.3823086880003432\n",
      "Fit LogisticRegression fold 4\n",
      "0.38821932243603624\n",
      "Fit LogisticRegression fold 5\n",
      "0.37126160279732473\n",
      "    cross_score: 0.38032\n",
      "Fit RandomForestClassifier fold 1\n",
      "0.6683274533777285\n",
      "Fit RandomForestClassifier fold 2\n",
      "0.6574936381540525\n",
      "Fit RandomForestClassifier fold 3\n",
      "0.6602220610255635\n",
      "Fit RandomForestClassifier fold 4\n",
      "0.6704147476213111\n",
      "Fit RandomForestClassifier fold 5\n",
      "0.6521973678600967\n",
      "    cross_score: 0.66173\n"
     ]
    }
   ],
   "source": [
    "cross_val_model(X, y, clf_A)\n",
    "cross_val_model(X, y, clf_B)\n",
    "# cross_val_model(X, y, clf_C)\n",
    "cross_val_model(X, y, clf_D)\n",
    "cross_val_model(X, y, clf_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_model(X, y, clf_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(clf_E.feature_importances_, index=X.columns).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {'C': [0.01, 0.1, 1, 10, 100],\n",
    "             'degree': [1,2,3,4,5],\n",
    "             'kernel':  ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = SVM()\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score, pos_label='yes')\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# XGBoost params\n",
    "xgb_params = {}\n",
    "xgb_params['learning_rate'] = 0.2\n",
    "xgb_params['n_estimators'] = 10\n",
    "xgb_params['max_depth'] = 4\n",
    "xgb_params['subsample'] = 0.9\n",
    "xgb_params['colsample_bytree'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(['TARGET', 'ID', 'strange_in_TARGET'], axis=1)\n",
    "y = train['TARGET']\n",
    "X_test = test.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "XGB_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit XGBClassifier fold 1\n",
      "Fit XGBClassifier fold 2\n",
      "Fit XGBClassifier fold 3\n",
      "Fit XGBClassifier fold 4\n",
      "Fit XGBClassifier fold 5\n",
      "    cross_score: 0.81420\n"
     ]
    }
   ],
   "source": [
    "cross_val_model(X, y, XGB_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.9, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.fit(X,y, eval_metric=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ypred = XGB_model.predict_proba(test.drop('ID', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test['ID'].values\n",
    "sub['target'] = ypred[:,1]\n",
    "sub.to_csv('baselineXGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NN_Classifier = MLPClassifier(hidden_layer_sizes=(500, 500, 250, 100, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500, 500, 250, 100, 50),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ypred2 = NN_Classifier.predict_proba(test.drop('ID', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub2 = pd.DataFrame()\n",
    "sub2['id'] = test['ID'].values\n",
    "sub2['target'] = ypred2[:,1]\n",
    "sub2.to_csv('baselineNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Support Vector Classifier with kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#RandomForest params\n",
    "rf_params = {}\n",
    "rf_params['n_estimators'] = 200\n",
    "rf_params['max_depth'] = 6\n",
    "rf_params['min_samples_split'] = 70\n",
    "rf_params['min_samples_leaf'] = 30\n",
    "\n",
    "clf_F = RandomForestClassifier(**rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train.replace([np.inf, -np.inf], np.nan).dropna().drop('TARGET', axis=1)\n",
    "y = train.dropna()['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cross_val_model(X, y, clf_F) # unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Test the baseline with unprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v = 'var5'\n",
    "sns.pairplot(train[meta.loc[(meta.d_type=='float') & (meta.variable==v) & meta.keep].name]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.FacetGrid(train, hue=\"TARGET\", size=6).map(sns.kdeplot, \"var15\").add_legend()\n",
    "plt.title('Unhappy customers are slightly older');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature correlation and importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v = 'var1'\n",
    "display(meta.loc[meta.variable==v].set_index('name'))\n",
    "sns.pairplot(train[meta.loc[meta.variable==v]['name']]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"Regressor to check features importance (as done in customer_segments project).\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\n",
    "variable = 'ind_var8_0'\n",
    "new_data = data.drop(feature, axis=1)\n",
    "\n",
    "# TODO: Split the data into training and testing sets(0.25) using the given feature as the target\n",
    "# Set a random state.\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data, data[feature], test_size = 0.25, random_state=3)\n",
    "\n",
    "# TODO: Create a decision tree regressor and fit it to the training set\n",
    "regressor = DecisionTreeRegressor(random_state=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Report the score of the prediction using the testing set\n",
    "score = regressor.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train[feature_names].corr()\n",
    "\n",
    "ver se da pra estimar features a partir de outras. parece q tem feature q eh bucket de outra\n",
    "ver se da pra estimar integers e binary a partir dos floats\n",
    "\n",
    "def get_feature_importance_df(feature_importances, \n",
    "                              column_names, \n",
    "                              top_n=25):\n",
    "    imp_dict = dict(zip(column_names, \n",
    "                        feature_importances))\n",
    "    top_features = sorted(imp_dict, \n",
    "                          key=imp_dict.get, \n",
    "                          reverse=True)[0:top_n]\n",
    "    top_importances = [imp_dict[feature] for feature \n",
    "                          in top_features]\n",
    "    df = pd.DataFrame(data={'feature': top_features, \n",
    "                            'importance': top_importances})\n",
    "    return df\n",
    "\n",
    "\n",
    "feature_importance = get_feature_importance_df(rf.feature_importances_, features)\n",
    "feature_importance\n",
    "\n",
    "# fig,ax = plt.subplots()\n",
    "# fig.set_size_inches(20,10)\n",
    "# sns.barplot(data=feature_importance[:10],x=\"feature\",y=\"importance\",ax=ax,color=default_color,)\n",
    "# ax.set(xlabel='Variable name', ylabel='Importance',title=\"Variable importances\");\n",
    "\n",
    "# sns.barplot(data=feature_importance[:10],x=\"feature\",y=\"importance\");\n",
    "\n",
    "\n",
    "# Testing feature importantes with Random Forest\n",
    "\n",
    "train.fillna(-1, inplace=True)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150,\n",
    "                            max_depth=8,\n",
    "                            min_samples_leaf=4,\n",
    "                            max_features=0.2,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "\n",
    "rf.fit(train.drop(['target'],axis=1), train.target)\n",
    "features = train.drop(['target'],axis=1).columns.values\n",
    "print(\"----- Training Done -----\")\n",
    "\n",
    "\n",
    "# Continuous features analysis\n",
    "\n",
    "col_internval\n",
    "\n",
    "corr_m = train[col_internval].corr()\n",
    "corr_m\n",
    "\n",
    "plt.figure(figsize=(18,16))\n",
    "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n",
    "sns.heatmap(train[col_internval].corr(),\n",
    "            linewidths=0.1,\n",
    "            vmax=1.0,\n",
    "            square=True,\n",
    "            cmap=colormap,\n",
    "            linecolor='white',\n",
    "            annot=True);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"Regressor to check features importance (as done in customer_segments project).\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\n",
    "feature = 'Delicatessen'\n",
    "new_data = data.drop(feature, axis=1)\n",
    "\n",
    "# TODO: Split the data into training and testing sets(0.25) using the given feature as the target\n",
    "# Set a random state.\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data, data[feature], test_size = 0.25, random_state=3)\n",
    "\n",
    "# TODO: Create a decision tree regressor and fit it to the training set\n",
    "regressor = DecisionTreeRegressor(random_state=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Report the score of the prediction using the testing set\n",
    "score = regressor.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train[variables['var7']]);\n",
    "sns.pairplot(train[variables['var8']]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v = 'saldo_medio_var13_largo_ult3'\n",
    "sns.kdeplot(train[v].apply(np.log));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(train.var38.apply(lambda x: np.log(x+1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train['var38']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train['var38'].apply(np.log));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(train.dtypes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.countplot('TARGET',data=train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**GridsearchCV** (http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV)\n",
    "\n",
    "sklearn.grid_search.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’)[source]\n",
    "\n",
    "cv : int, cross-validation generator or an iterable, optional\n",
    "\n",
    "Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "None, to use the default 3-fold cross-validation,\n",
    "integer, to specify the number of folds.\n",
    "An object to be used as a cross-validation generator.\n",
    "An iterable yielding train/test splits.\n",
    "For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, sklearn.model_selection.StratifiedKFold is used. In all other cases, sklearn.model_selection.KFold is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\"oneHotEncode'em all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evolution features (relationships within variable attrivbutes): imp_op_var39_comer_ult1 > imp_op_var39_comer_ult3\n",
    "\n",
    "import itertools\n",
    "for variable in set(meta.variable):\n",
    "    v_combs = [list(i) for i in itertools.combinations(meta[meta.variable==variable]['name'], 2)]\n",
    "    for comb in v_combs:\n",
    "        train[comb[0]+'GT'+comb[1]] = train[comb[0]] > train[comb[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.shapex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "clf_A = NaiveBayes()\n",
    "clf_B = DecisionTree()\n",
    "clf_C = SVM()\n",
    "clf_D = LogisticRegression()\n",
    "clf_E = RandomForestClassifier()\n",
    "\n",
    "X = train.drop('TARGET', axis=1)\n",
    "X = train.drop(['TARGET', 'ID'], axis=1)\n",
    "y = train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cross_val_model(X, y, clf_A)\n",
    "cross_val_model(X, y, clf_B)\n",
    "# cross_val_model(X, y, clf_C)\n",
    "cross_val_model(X, y, clf_D)\n",
    "cross_val_model(X, y, clf_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3bdb4c58348d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Processed feature columns ({} total features):\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the student data and converts non-numeric binary variables into\n",
    "        binary (0/1) variables. Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        \n",
    "        # If data type is non-numeric, replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns))\n",
    "\n",
    "# scaling features\n",
    "# TODO: Scale the data using the natural logarithm\n",
    "log_data = data.apply(np.log)\n",
    "\n",
    "# TODO: Scale the sample data using the natural logarithm\n",
    "log_samples = samples.apply(np.log)\n",
    "\n",
    "# Produce a scatter matrix for each pair of newly-transformed features\n",
    "pd.plotting.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Rascunho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Unauthorized: you must download an API key from https://www.kaggle.com/<username>/account\nThen put kaggle.json in the folder /Users/vieiraad/.kaggle",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Unauthorized: you must download an API key from https://www.kaggle.com/<username>/account\nThen put kaggle.json in the folder /Users/vieiraad/.kaggle\n"
     ]
    }
   ],
   "source": [
    "### Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import kaggle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB as NaiveBayes\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# import itertools\n",
    "# import missingno as msno\n",
    "# import gc\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB as NaiveBayes\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.svm import SVC as SVM\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.mixture import GaussianMixture as GM\n",
    "# from hyperopt.pyll.base import scope\n",
    "# from hyperopt.pyll.stochastic import sample\n",
    "# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_meta(df):\n",
    "    data = []\n",
    "    for col in df.columns:\n",
    "        # Defining the role\n",
    "        if col == 'TARGET':\n",
    "            role = 'target'\n",
    "        elif col == 'ID':\n",
    "            role = 'id'\n",
    "        else:\n",
    "            role = 'input'\n",
    "            \n",
    "        # Defining variable which is related to and attributes of this variable\n",
    "        variable = ''.join(filter(lambda x: any(v in x for v in ['var', 'ID', 'TARGET']), col.split('_')))\n",
    "        attribute = col.replace(variable, '').replace('__', '_')\n",
    "        if attribute in ['', variable]: attribute = 'root'\n",
    "        variable = variable.replace('cte', '')\n",
    "\n",
    "        # Defining nunique \n",
    "        nunique = df[col].nunique()\n",
    "\n",
    "#         # Initialize keep to True for all variables except for id or useless variables (nunque < 2)\n",
    "#         keep = True\n",
    "#         if role == 'id' or nunique<2:\n",
    "#             keep = False\n",
    "        # Measuring Mode Strength -- ver isso para os features integers\n",
    "\n",
    "        # Defining the data type \n",
    "        if nunique == 2:\n",
    "            dtype = 'binary'\n",
    "        elif df[col].dtype == type(2):\n",
    "            dtype = 'integer'\n",
    "        elif df[col].dtype == type(0.1):\n",
    "            dtype = 'float'\n",
    "\n",
    "        # Creating a Dict that contains all the metadata for the variable\n",
    "        col_dict = {\n",
    "            'name'     : col,\n",
    "            'role'     : role,\n",
    "            'variable' : variable,\n",
    "            'attribute': attribute,\n",
    "            'n_unique'  : nunique,\n",
    "            'd_type'     : dtype,\n",
    "#             'keep'     : keep\n",
    "        }\n",
    "        \n",
    "        data.append(col_dict)\n",
    "#     meta = pd.DataFrame(data, columns=['name', 'role', 'variable', 'attribute','n_unique', 'd_type', 'keep'])\n",
    "    meta = pd.DataFrame(data, columns=['name', 'role', 'variable', 'attribute','n_unique', 'd_type'])\n",
    "    return meta\n",
    "\n",
    "meta = get_meta(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6e55f7007fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.kaggle.com/account/login?ReturnUrl=%2fc%2f'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcompetition_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%2fpublicleaderboarddata.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#call it once to get the request verification cookie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' password'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__RequestVerificationToken'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__RequestVerificationToken'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         settings = self.merge_environment_settings(\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         )\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mmerge_environment_settings\u001b[0;34m(self, url, proxies, stream, verify, cert)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# Set environment's proxies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0menv_proxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_environ_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_proxies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/site-packages/requests/utils.pyc\u001b[0m in \u001b[0;36mget_environ_proxies\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \"\"\"\n\u001b[0;32m--> 617\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mshould_bypass_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/site-packages/requests/utils.pyc\u001b[0m in \u001b[0;36mshould_bypass_proxies\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# legitimate problems.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mbypass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy_bypass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mbypass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mproxy_bypass\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproxy_bypass_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mproxy_bypass_macosx_sysconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetproxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vieiraad/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mproxy_bypass_macosx_sysconf\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m   1451\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m         \u001b[0mproxy_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_proxy_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0;31m# Check for simple host names:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "username = ''\n",
    "password = ''\n",
    "competition_id='3136' #the code for titanic\n",
    "\n",
    "session = requests.Session()\n",
    "get_me = 'https://www.kaggle.com/account/login?ReturnUrl=%2fc%2f' + competition_id + '%2fpublicleaderboarddata.zip'\n",
    "response = session.get(get_me) #call it once to get the request verification cookie\n",
    "payload = {'username': username,' password':password, '__RequestVerificationToken': session.cookies.get('__RequestVerificationToken')}\n",
    "r = session.post(get_me, data=payload)\n",
    "with open('the-zip.zip', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "'https://www.kaggle.com/account/login?ReturnUrl=%2fc%2fsantander-customer-satisfaction%2fpublicleaderboarddata.zip'\n",
    "'https://www.kaggle.com/account/login?ReturnUrl=%2fc%2fsantander-customer-satisfaction%2fprivateleaderboarddata.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=var38                              False\n",
       "num_var20_0                         True\n",
       "num_meses_var13_largo_ult3          True\n",
       "var15                              False\n",
       "saldo_medio_var8_hace2             False\n",
       "ind_var30                           True\n",
       "num_var33_0                      ...eses_var13_corto_ult3_oh_2     True\n",
       "num_meses_var13_corto_ult3_oh_1     True\n",
       "Length: 99, dtype: bool,\n",
       "       dtype=<type 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features=((train.nunique()<5) & (train.dtypes==int)))\n",
    "ohe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \n",
    "OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>, handle_unknown='error', n_values='auto', sparse=True)\n",
    "enc.n_values_\n",
    "enc.feature_indices_\n",
    "enc.transform([[0, 1, 1]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scores = pd.read_csv('../../Downloads/santander-customer-satisfaction-publicleaderboard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAF5CAYAAADOExOGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPmbB8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136adb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(scores.loc[scores.Score>0.5].Score);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TeamId</th>\n",
       "      <th>TeamName</th>\n",
       "      <th>SubmissionDate</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285018</td>\n",
       "      <td>Alexander Makeev</td>\n",
       "      <td>2016-03-02 20:33:18</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>286451</td>\n",
       "      <td>#1 Leustagos</td>\n",
       "      <td>2016-03-02 20:37:47</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285018</td>\n",
       "      <td>Alexander Makeev</td>\n",
       "      <td>2016-03-02 20:59:34</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285027</td>\n",
       "      <td>Liam Bressler</td>\n",
       "      <td>2016-03-02 21:12:23</td>\n",
       "      <td>0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285043</td>\n",
       "      <td>Michael Pawlus</td>\n",
       "      <td>2016-03-02 21:25:19</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>285044</td>\n",
       "      <td>DL</td>\n",
       "      <td>2016-03-02 21:27:03</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>285045</td>\n",
       "      <td>Anthony P.</td>\n",
       "      <td>2016-03-02 21:30:23</td>\n",
       "      <td>0.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>285045</td>\n",
       "      <td>Anthony P.</td>\n",
       "      <td>2016-03-02 21:31:01</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>285046</td>\n",
       "      <td>clustifier</td>\n",
       "      <td>2016-03-02 21:35:56</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>285048</td>\n",
       "      <td>Bikash</td>\n",
       "      <td>2016-03-02 21:36:59</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>285027</td>\n",
       "      <td>Liam Bressler</td>\n",
       "      <td>2016-03-02 21:39:25</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>285027</td>\n",
       "      <td>Liam Bressler</td>\n",
       "      <td>2016-03-02 21:44:08</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>285052</td>\n",
       "      <td>Robert Martin</td>\n",
       "      <td>2016-03-02 21:45:08</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>285050</td>\n",
       "      <td>BlackCore</td>\n",
       "      <td>2016-03-02 21:48:02</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>285057</td>\n",
       "      <td>PaulCrease</td>\n",
       "      <td>2016-03-02 21:50:44</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>285029</td>\n",
       "      <td>massil</td>\n",
       "      <td>2016-03-02 21:51:20</td>\n",
       "      <td>0.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>285058</td>\n",
       "      <td>K.G.J.</td>\n",
       "      <td>2016-03-02 21:53:06</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>285062</td>\n",
       "      <td>Lawrence Chernin</td>\n",
       "      <td>2016-03-02 21:55:42</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>285027</td>\n",
       "      <td>Liam Bressler</td>\n",
       "      <td>2016-03-02 21:58:43</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>285029</td>\n",
       "      <td>massil</td>\n",
       "      <td>2016-03-02 22:06:29</td>\n",
       "      <td>0.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>285668</td>\n",
       "      <td>Rolling Stones (Canât Get No [Customer] Sati...</td>\n",
       "      <td>2016-03-02 22:06:41</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>285070</td>\n",
       "      <td>Jason Miller</td>\n",
       "      <td>2016-03-02 22:07:43</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>285052</td>\n",
       "      <td>Robert Martin</td>\n",
       "      <td>2016-03-02 22:15:59</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>285080</td>\n",
       "      <td>thecatwithfuturefeet</td>\n",
       "      <td>2016-03-02 22:20:28</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>285018</td>\n",
       "      <td>Alexander Makeev</td>\n",
       "      <td>2016-03-02 22:24:19</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>285052</td>\n",
       "      <td>Robert Martin</td>\n",
       "      <td>2016-03-02 22:25:44</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>285086</td>\n",
       "      <td>â</td>\n",
       "      <td>2016-03-02 22:29:13</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>285052</td>\n",
       "      <td>Robert Martin</td>\n",
       "      <td>2016-03-02 22:33:59</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>285091</td>\n",
       "      <td>vbk</td>\n",
       "      <td>2016-03-02 22:36:56</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>285090</td>\n",
       "      <td>Sterby</td>\n",
       "      <td>2016-03-02 22:38:11</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28122</th>\n",
       "      <td>300380</td>\n",
       "      <td>å±±å½±å¹¸å½¦</td>\n",
       "      <td>2016-07-10 20:45:55</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28123</th>\n",
       "      <td>289643</td>\n",
       "      <td>CeShine Lee</td>\n",
       "      <td>2016-08-07 02:06:41</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28124</th>\n",
       "      <td>303886</td>\n",
       "      <td>Jongheon Jeong</td>\n",
       "      <td>2016-08-10 10:56:36</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28125</th>\n",
       "      <td>299662</td>\n",
       "      <td>qiull</td>\n",
       "      <td>2016-09-10 05:41:10</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28126</th>\n",
       "      <td>299662</td>\n",
       "      <td>qiull</td>\n",
       "      <td>2016-09-11 14:13:42</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28127</th>\n",
       "      <td>298309</td>\n",
       "      <td>GoGoGo</td>\n",
       "      <td>2016-09-13 12:13:49</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28128</th>\n",
       "      <td>298309</td>\n",
       "      <td>GoGoGo</td>\n",
       "      <td>2016-09-13 12:36:33</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28129</th>\n",
       "      <td>291225</td>\n",
       "      <td>Clark Djilo</td>\n",
       "      <td>2016-10-30 18:51:37</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28130</th>\n",
       "      <td>303654</td>\n",
       "      <td>Cpumar</td>\n",
       "      <td>2016-11-01 09:34:11</td>\n",
       "      <td>0.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28131</th>\n",
       "      <td>309956</td>\n",
       "      <td>Saiyan</td>\n",
       "      <td>2016-11-06 06:12:40</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28132</th>\n",
       "      <td>304509</td>\n",
       "      <td>MexZer0</td>\n",
       "      <td>2016-11-10 07:51:21</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28133</th>\n",
       "      <td>286434</td>\n",
       "      <td>Ahmet Sakbay</td>\n",
       "      <td>2016-12-27 08:34:07</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28134</th>\n",
       "      <td>295885</td>\n",
       "      <td>boobayooba</td>\n",
       "      <td>2017-01-11 18:26:59</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28135</th>\n",
       "      <td>295885</td>\n",
       "      <td>boobayooba</td>\n",
       "      <td>2017-01-11 18:55:58</td>\n",
       "      <td>0.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28136</th>\n",
       "      <td>299258</td>\n",
       "      <td>Roberto Paredes</td>\n",
       "      <td>2017-02-04 22:38:59</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28137</th>\n",
       "      <td>299258</td>\n",
       "      <td>Roberto Paredes</td>\n",
       "      <td>2017-02-08 11:30:34</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28138</th>\n",
       "      <td>309959</td>\n",
       "      <td>MirageChung</td>\n",
       "      <td>2017-03-05 12:42:45</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28139</th>\n",
       "      <td>309959</td>\n",
       "      <td>MirageChung</td>\n",
       "      <td>2017-03-06 13:24:09</td>\n",
       "      <td>0.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28140</th>\n",
       "      <td>302883</td>\n",
       "      <td>aamir121a</td>\n",
       "      <td>2017-04-18 08:09:21</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28141</th>\n",
       "      <td>293622</td>\n",
       "      <td>JorgePorca</td>\n",
       "      <td>2017-05-17 14:44:30</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28142</th>\n",
       "      <td>297674</td>\n",
       "      <td>Darshan Bagul</td>\n",
       "      <td>2017-06-02 01:16:24</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28143</th>\n",
       "      <td>304064</td>\n",
       "      <td>Optima</td>\n",
       "      <td>2017-07-08 13:38:52</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28144</th>\n",
       "      <td>312681</td>\n",
       "      <td>mariopm</td>\n",
       "      <td>2017-07-09 14:34:04</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28145</th>\n",
       "      <td>285664</td>\n",
       "      <td>CJCJ</td>\n",
       "      <td>2017-08-02 16:58:39</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28146</th>\n",
       "      <td>293622</td>\n",
       "      <td>JorgePorca</td>\n",
       "      <td>2017-08-12 22:57:53</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28147</th>\n",
       "      <td>293622</td>\n",
       "      <td>JorgePorca</td>\n",
       "      <td>2017-08-13 09:07:07</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28148</th>\n",
       "      <td>293622</td>\n",
       "      <td>JorgePorca</td>\n",
       "      <td>2017-08-21 12:01:00</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28149</th>\n",
       "      <td>290405</td>\n",
       "      <td>HowardPaget</td>\n",
       "      <td>2017-11-14 11:07:13</td>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28150</th>\n",
       "      <td>294845</td>\n",
       "      <td>DmitriyKamsha</td>\n",
       "      <td>2017-12-24 16:49:09</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28151</th>\n",
       "      <td>314414</td>\n",
       "      <td>Michael Hartman</td>\n",
       "      <td>2018-04-24 19:49:40</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28152 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TeamId                                           TeamName  \\\n",
       "0      285018                                   Alexander Makeev   \n",
       "1      286451                                       #1 Leustagos   \n",
       "2      285018                                   Alexander Makeev   \n",
       "3      285027                                      Liam Bressler   \n",
       "4      285043                                     Michael Pawlus   \n",
       "5      285044                                                 DL   \n",
       "6      285045                                         Anthony P.   \n",
       "7      285045                                         Anthony P.   \n",
       "8      285046                                         clustifier   \n",
       "9      285048                                             Bikash   \n",
       "10     285027                                      Liam Bressler   \n",
       "11     285027                                      Liam Bressler   \n",
       "12     285052                                      Robert Martin   \n",
       "13     285050                                          BlackCore   \n",
       "14     285057                                         PaulCrease   \n",
       "15     285029                                             massil   \n",
       "16     285058                                             K.G.J.   \n",
       "17     285062                                   Lawrence Chernin   \n",
       "18     285027                                      Liam Bressler   \n",
       "19     285029                                             massil   \n",
       "20     285668  Rolling Stones (Canât Get No [Customer] Sati...   \n",
       "21     285070                                       Jason Miller   \n",
       "22     285052                                      Robert Martin   \n",
       "23     285080                               thecatwithfuturefeet   \n",
       "24     285018                                   Alexander Makeev   \n",
       "25     285052                                      Robert Martin   \n",
       "26     285086                                                â   \n",
       "27     285052                                      Robert Martin   \n",
       "28     285091                                                vbk   \n",
       "29     285090                                             Sterby   \n",
       "...       ...                                                ...   \n",
       "28122  300380                                       å±±å½±å¹¸å½¦   \n",
       "28123  289643                                        CeShine Lee   \n",
       "28124  303886                                     Jongheon Jeong   \n",
       "28125  299662                                              qiull   \n",
       "28126  299662                                              qiull   \n",
       "28127  298309                                             GoGoGo   \n",
       "28128  298309                                             GoGoGo   \n",
       "28129  291225                                        Clark Djilo   \n",
       "28130  303654                                             Cpumar   \n",
       "28131  309956                                             Saiyan   \n",
       "28132  304509                                            MexZer0   \n",
       "28133  286434                                       Ahmet Sakbay   \n",
       "28134  295885                                         boobayooba   \n",
       "28135  295885                                         boobayooba   \n",
       "28136  299258                                    Roberto Paredes   \n",
       "28137  299258                                    Roberto Paredes   \n",
       "28138  309959                                        MirageChung   \n",
       "28139  309959                                        MirageChung   \n",
       "28140  302883                                          aamir121a   \n",
       "28141  293622                                         JorgePorca   \n",
       "28142  297674                                      Darshan Bagul   \n",
       "28143  304064                                             Optima   \n",
       "28144  312681                                            mariopm   \n",
       "28145  285664                                               CJCJ   \n",
       "28146  293622                                         JorgePorca   \n",
       "28147  293622                                         JorgePorca   \n",
       "28148  293622                                         JorgePorca   \n",
       "28149  290405                                        HowardPaget   \n",
       "28150  294845                                      DmitriyKamsha   \n",
       "28151  314414                                    Michael Hartman   \n",
       "\n",
       "            SubmissionDate  Score  \n",
       "0      2016-03-02 20:33:18  0.823  \n",
       "1      2016-03-02 20:37:47  0.838  \n",
       "2      2016-03-02 20:59:34  0.833  \n",
       "3      2016-03-02 21:12:23  0.820  \n",
       "4      2016-03-02 21:25:19  0.833  \n",
       "5      2016-03-02 21:27:03  0.837  \n",
       "6      2016-03-02 21:30:23  0.506  \n",
       "7      2016-03-02 21:31:01  0.833  \n",
       "8      2016-03-02 21:35:56  0.839  \n",
       "9      2016-03-02 21:36:59  0.827  \n",
       "10     2016-03-02 21:39:25  0.821  \n",
       "11     2016-03-02 21:44:08  0.834  \n",
       "12     2016-03-02 21:45:08  0.821  \n",
       "13     2016-03-02 21:48:02  0.833  \n",
       "14     2016-03-02 21:50:44  0.766  \n",
       "15     2016-03-02 21:51:20  0.646  \n",
       "16     2016-03-02 21:53:06  0.831  \n",
       "17     2016-03-02 21:55:42  0.833  \n",
       "18     2016-03-02 21:58:43  0.834  \n",
       "19     2016-03-02 22:06:29  0.659  \n",
       "20     2016-03-02 22:06:41  0.833  \n",
       "21     2016-03-02 22:07:43  0.832  \n",
       "22     2016-03-02 22:15:59  0.823  \n",
       "23     2016-03-02 22:20:28  0.840  \n",
       "24     2016-03-02 22:24:19  0.838  \n",
       "25     2016-03-02 22:25:44  0.826  \n",
       "26     2016-03-02 22:29:13  0.833  \n",
       "27     2016-03-02 22:33:59  0.826  \n",
       "28     2016-03-02 22:36:56  0.833  \n",
       "29     2016-03-02 22:38:11  0.353  \n",
       "...                    ...    ...  \n",
       "28122  2016-07-10 20:45:55  0.838  \n",
       "28123  2016-08-07 02:06:41  0.842  \n",
       "28124  2016-08-10 10:56:36  0.839  \n",
       "28125  2016-09-10 05:41:10  0.834  \n",
       "28126  2016-09-11 14:13:42  0.835  \n",
       "28127  2016-09-13 12:13:49  0.836  \n",
       "28128  2016-09-13 12:36:33  0.836  \n",
       "28129  2016-10-30 18:51:37  0.841  \n",
       "28130  2016-11-01 09:34:11  0.751  \n",
       "28131  2016-11-06 06:12:40  0.838  \n",
       "28132  2016-11-10 07:51:21  0.842  \n",
       "28133  2016-12-27 08:34:07  0.842  \n",
       "28134  2017-01-11 18:26:59  0.766  \n",
       "28135  2017-01-11 18:55:58  0.768  \n",
       "28136  2017-02-04 22:38:59  0.816  \n",
       "28137  2017-02-08 11:30:34  0.837  \n",
       "28138  2017-03-05 12:42:45  0.542  \n",
       "28139  2017-03-06 13:24:09  0.751  \n",
       "28140  2017-04-18 08:09:21  0.840  \n",
       "28141  2017-05-17 14:44:30  0.669  \n",
       "28142  2017-06-02 01:16:24  0.842  \n",
       "28143  2017-07-08 13:38:52  0.817  \n",
       "28144  2017-07-09 14:34:04  0.840  \n",
       "28145  2017-08-02 16:58:39  0.839  \n",
       "28146  2017-08-12 22:57:53  0.693  \n",
       "28147  2017-08-13 09:07:07  0.829  \n",
       "28148  2017-08-21 12:01:00  0.829  \n",
       "28149  2017-11-14 11:07:13  0.520  \n",
       "28150  2017-12-24 16:49:09  0.838  \n",
       "28151  2018-04-24 19:49:40  0.841  \n",
       "\n",
       "[28152 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PipelineHelper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d15bf3d69f8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m pipe = Pipeline([\n\u001b[0;32m----> 6\u001b[0;31m     ('scaler', PipelineHelper([\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'minmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PipelineHelper' is not defined"
     ]
    }
   ],
   "source": [
    "### Pipeline nos modelos sem parametrizar, mas testando scalers?\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "pipe = Pipeline(steps=\n",
    "    [\n",
    "    ('scaler', PipelineHelper([\n",
    "        ('std', StandardScaler()),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "        ('max', MaxAbsScaler()),\n",
    "    ])),\n",
    "    ('classifier', PipelineHelper([\n",
    "        ('nb', GaussianNB()),\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('dt', DecisionTreeClassifier()),\n",
    "        ('sv', SVC()),\n",
    "        ('mlp', MLPClassifier()),\n",
    "        ('rf', RandomForestClassifier()),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "# params = {\n",
    "#     'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "#         'std__with_mean': [True, False],\n",
    "#         'std__with_std': [True, False],\n",
    "#         'max__copy': [True],  # just for displaying\n",
    "#     }),\n",
    "#     'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
    "#         'svm__C': [0.1, 1.0],\n",
    "#         'rf__n_estimators': [100, 20],\n",
    "#     })\n",
    "# }\n",
    "# grid = GridSearchCV(pipe, params, scoring=roc_auc_scorer, verbose=1)\n",
    "# grid.fit(X, y)\n",
    "# print(grid.best_params_)\n",
    "# print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Implementation: Outlier Detection\n",
    "Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method for identfying outliers](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/): An *outlier step* is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.\n",
    "\n",
    "In the code block below, you will need to implement the following:\n",
    " - Assign the value of the 25th percentile for the given feature to `Q1`. Use `np.percentile` for this.\n",
    " - Assign the value of the 75th percentile for the given feature to `Q3`. Again, use `np.percentile`.\n",
    " - Assign the calculation of an outlier step for the given feature to `step`.\n",
    " - Optionally remove data points from the dataset by adding indices to the `outliers` list.\n",
    "\n",
    "**NOTE:** If you choose to remove any outliers, ensure that the sample data does not contain any of these points!  \n",
    "Once you have performed this implementation, the dataset will be stored in the variable `good_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# For each feature find the data points with extreme high or low values\n",
    "outliers = []\n",
    "for feature in train.columns:\n",
    "    \n",
    "    # Q1 (25th percentile of the data) for the given feature\n",
    "    Q1 = np.percentile(train[feature], 25)\n",
    "    \n",
    "    # Q3 (75th percentile of the data) for the given feature\n",
    "    Q3 = np.percentile(train[feature], 75)\n",
    "    \n",
    "    # Interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "    step = 1.5 * (Q3 - Q1)\n",
    "    outliers+=list(train[~((train[feature] >= Q1 - step) & (train[feature] <= Q3 + step))].index)\n",
    "    \n",
    "#     # Display the outliers\n",
    "#     print(\"Data points considered outliers for the feature '{}':\".format(feature))\n",
    "#     display(train[~((train[feature] >= Q1 - step) & (train[feature] <= Q3 + step))])\n",
    "    \n",
    "# # OPTIONAL: Select the indices for data points you wish to remove\n",
    "# outliers = []\n",
    "# for feature in log_data.keys():\n",
    "#     Q1 = np.percentile(log_data[feature], 25)\n",
    "#     Q3 = np.percentile(log_data[feature], 75)\n",
    "#     step = 1.5 * (Q3 - Q1)\n",
    "#     outliers += list(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))].index)\n",
    "\n",
    "# # Remove the outliers, if any were specified\n",
    "# good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48866"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print len(set(outliers))\n",
    "double_outliers = [item for item in outliers if outliers.count(item)>1]\n",
    "print len(set(double_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SKLearn Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://minerandodados.com.br/index.php/2018/05/21/feature-selection-bala-de-prata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_meta(df):\n",
    "    data = []\n",
    "    for col in df.columns:\n",
    "        # Defining the role\n",
    "        if col == 'TARGET':\n",
    "            role = 'target'\n",
    "        elif col == 'ID':\n",
    "            role = 'id'\n",
    "        else:\n",
    "            role = 'input'\n",
    "            \n",
    "        # Defining variable which is related to and attributes of this variable\n",
    "        variable = ''.join(filter(lambda x: any(v in x for v in ['var', 'ID', 'TARGET']), col.split('_')))\n",
    "        attribute = col.replace(variable, '').replace('__', '_')\n",
    "        if attribute in ['', variable]: attribute = 'root'\n",
    "        variable = variable.replace('cte', '')\n",
    "\n",
    "        # Defining nunique \n",
    "        nunique = df[col].nunique()\n",
    "\n",
    "#         # Initialize keep to True for all variables except for id or useless variables (nunque < 2)\n",
    "#         keep = True\n",
    "#         if role == 'id' or nunique<2:\n",
    "#             keep = False\n",
    "        # Measuring Mode Strength -- ver isso para os features integers\n",
    "\n",
    "        # Defining the data type \n",
    "        if nunique == 2:\n",
    "            dtype = 'binary'\n",
    "        elif df[col].dtype == type(2):\n",
    "            dtype = 'integer'\n",
    "        elif df[col].dtype == type(0.1):\n",
    "            dtype = 'float'\n",
    "\n",
    "        # Creating a Dict that contains all the metadata for the variable\n",
    "        col_dict = {\n",
    "            'name'     : col,\n",
    "            'role'     : role,\n",
    "            'variable' : variable,\n",
    "            'attribute': attribute,\n",
    "            'n_unique'  : nunique,\n",
    "            'd_type'     : dtype,\n",
    "#             'keep'     : keep\n",
    "        }\n",
    "        \n",
    "        data.append(col_dict)\n",
    "#     meta = pd.DataFrame(data, columns=['name', 'role', 'variable', 'attribute','n_unique', 'd_type', 'keep'])\n",
    "    meta = pd.DataFrame(data, columns=['name', 'role', 'variable', 'attribute','n_unique', 'd_type'])\n",
    "    return meta\n",
    "\n",
    "meta = get_meta(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_meses_var13_largo_ult3\n",
      "[0, 3, 2, 1]\n",
      "\n",
      "num_var33_0\n",
      "[0, 3, 6, 12]\n",
      "\n",
      "num_meses_var5_ult3\n",
      "[3, 0, 2, 1]\n",
      "\n",
      "num_aport_var13_hace3\n",
      "[0, 3, 6, 9, 12, 18, 24, 15]\n",
      "\n",
      "num_var31_0\n",
      "[0, 3, 6, 9, 12, 15, 18, 24, 21, 66, 36, 27]\n",
      "\n",
      "num_meses_var13_corto_ult3\n",
      "[0, 2, 3, 1]\n",
      "\n",
      "num_var24\n",
      "[0, 3, 6]\n",
      "\n",
      "num_reemb_var17_ult1\n",
      "[0, 3, 9, 6, 21, 12]\n",
      "\n",
      "num_meses_var44_ult3\n",
      "[0, 2, 1, 3]\n",
      "\n",
      "num_var33\n",
      "[0, 3, 6]\n",
      "\n",
      "num_venta_var44_ult1\n",
      "[0, 3, 6, 12, 9, 21, 15, 27, 45, 39, 24]\n",
      "\n",
      "num_meses_var17_ult3\n",
      "[0, 2, 1, 3]\n",
      "\n",
      "num_var13_largo_0\n",
      "[0, 3, 6, 9, 12, 21, 18, 15]\n",
      "\n",
      "num_compra_var44_ult1\n",
      "[0, 3, 6, 9, 12, 18, 24, 15, 51, 39, 27, 21]\n",
      "\n",
      "num_var32_0\n",
      "[0, 3, 6, 12, 9]\n",
      "\n",
      "num_var8_0\n",
      "[0, 3, 6]\n",
      "\n",
      "num_meses_var33_ult3\n",
      "[0, 2, 3, 1]\n",
      "\n",
      "num_var42\n",
      "[3, 0, 6, 9, 12, 15, 18]\n",
      "\n",
      "num_sal_var16_ult1\n",
      "[0, 3, 6, 9, 15, 12]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEFCAYAAAAsU2YoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWIqImJDCW0TEhBTeIiImpPAWETGh/w/+khu4vyqJdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a660ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.concat([train.drop('TARGET', axis=1), test])\n",
    "\n",
    "for c in df.loc[:,df.nunique()>2].select_dtypes('int'):\n",
    "    print(c)\n",
    "    print(list(df[c].value_counts().index))\n",
    "#     if list(train[c].value_counts().index) == list(test[c].value_counts().index):\n",
    "#         print('Same values for train and test set.')\n",
    "#     else:\n",
    "#         print('Different values on test set!!!!!!!!!')\n",
    "#         print(list(test[c].value_counts().index))\n",
    "    print()\n",
    "    if df[c].nunique()<5:\n",
    "        sns.distplot(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def values_sum(df):\n",
    "    df['values_sum'] = df[features].sum(axis=1).astype(float)\n",
    "#     if 'TARGET' in df.columns:\n",
    "#         df['values_sum'] = df.drop('TARGET', axis=1).sum(axis=1).astype(float)\n",
    "#     else:\n",
    "#         df['values_sum'] = df.sum(axis=1).astype(float)\n",
    "\n",
    "count_zeros(train)\n",
    "count_zeros(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Count zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_zeros(df):\n",
    "    df['zeros'] = (df[features]==0).sum(axis=1).astype(float)\n",
    "#     if 'TARGET' in df.columns:\n",
    "#         df['zeros'] = (df.drop('TARGET', axis=1)==0).sum(axis=1).astype(float)\n",
    "#     else:\n",
    "#         df['zeros'] = (df==0).sum(axis=1).astype(float)\n",
    "    \n",
    "count_zeros(train)\n",
    "count_zeros(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Value == Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_if_is_mode(df):\n",
    "    modes = df.mode()\n",
    "    for c in features:\n",
    "        df['is_mode_of_'+c] = (df[c]==modes[c][0]).astype(int)\n",
    "\n",
    "#     for c in df.columns:\n",
    "#         if c!= 'TARGET':\n",
    "#             df['is_mode_of_'+c] = (df[c]==modes[c][0]).astype(int)\n",
    "\n",
    "\n",
    "check_if_is_mode(train)\n",
    "check_if_is_mode(test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
